{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "h9jYIvibLELY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Can we use Bagging for regression problems ?\n",
        "\n",
        "ANS-> Yes, we can use Bagging for regression problems. Bagging, also known as Bootstrap Aggregating, is a popular ensemble learning technique that can be used for both classification and regression problems.\n",
        "\n",
        "In the context of regression, Bagging works by creating multiple instances of the training dataset using bootstrap sampling, training a separate model on each instance, and then combining the predictions of all the models to produce the final output.\n",
        "\n",
        "Here's how Bagging can be used for regression:\n",
        "\n",
        "1. Bootstrap Sampling: Create multiple instances of the training dataset by sampling with replacement. This is known as bootstrap sampling.\n",
        "2. Model Training: Train a separate model on each bootstrap sample. This can be any type of regression model, such as linear regression, decision trees, or support vector machines.\n",
        "3. Prediction: Make predictions on the test dataset using each of the trained models.\n",
        "4. Combining Predictions: Combine the predictions of all the models to produce the final output. This is typically done by taking the average of all the predictions.\n",
        "\n",
        "Bagging can help improve the performance of regression models in several ways:\n",
        "\n",
        "- Reducing Overfitting: By averaging the predictions of multiple models, Bagging can help reduce overfitting.\n",
        "- Improving Robustness: Bagging can help improve the robustness of regression models by reducing the impact of outliers and noisy data.\n",
        "- Handling Non-Linear Relationships: Bagging can be used with non-linear models, such as decision trees or support vector machines, to handle non-linear relationships between the features and target variable."
      ],
      "metadata": {
        "id": "olV4Cgb3LEQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "ANS-> Multiple model training and single model training are two different approaches used in machine learning to train models. Here are the key differences between them:\n",
        "\n",
        "Single Model Training:\n",
        "\n",
        "1. One Model: A single model is trained on the entire dataset.\n",
        "2. Simple: This approach is straightforward and easy to implement.\n",
        "3. Fast: Training a single model is generally faster than training multiple models.\n",
        "4. Limited: A single model may not capture all the complexities and nuances of the data.\n",
        "5. Overfitting: A single model may overfit the training data, resulting in poor performance on unseen data.\n",
        "\n",
        "Multiple Model Training (Ensemble Learning):\n",
        "\n",
        "1. Multiple Models: Multiple models are trained on different subsets of the data or using different algorithms.\n",
        "2. Complex: This approach is more complex and requires careful selection of models and hyperparameters.\n",
        "3. Slower: Training multiple models is generally slower than training a single model.\n",
        "4. Robust: Multiple models can capture different aspects of the data, resulting in a more robust and accurate prediction.\n",
        "5. Reduced Overfitting: Ensemble methods can reduce overfitting by averaging the predictions of multiple models.\n"
      ],
      "metadata": {
        "id": "Mg_7_ds9LESR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "ANS-> Feature randomness is a key concept in Random Forest, a popular ensemble learning algorithm. Here's a breakdown:\n",
        "\n",
        "What is feature randomness?\n",
        "\n",
        "In Random Forest, feature randomness refers to the random selection of features (or columns) from the dataset at each node of the decision tree. This randomness is introduced to reduce the correlation between trees and improve the overall accuracy of the model.\n",
        "\n",
        "How does feature randomness work?\n",
        "\n",
        "Here's a step-by-step explanation:\n",
        "\n",
        "1. Feature selection: At each node of the decision tree, a random subset of features is selected from the entire feature set.\n",
        "2. Splitting: The decision tree splits the data based on the selected features, using the best split criterion (e.g., Gini impurity or entropy).\n",
        "3. Tree growth: The decision tree grows by recursively splitting the data until a stopping criterion is reached (e.g., maximum depth or minimum samples per leaf).\n",
        "4. Multiple trees: Multiple decision trees are grown using different random subsets of features, resulting in a diverse set of trees.\n",
        "5. Voting: The predictions from each tree are combined using voting (for classification) or averaging (for regression) to produce the final output.\n",
        "\n",
        "Benefits of feature randomness\n",
        "\n",
        "Feature randomness in Random Forest offers several benefits:\n",
        "\n",
        "- Reduced correlation: By randomly selecting features at each node, the correlation between trees is reduced, resulting in a more diverse and robust ensemble.\n",
        "- Improved accuracy: Feature randomness helps to reduce overfitting and improves the overall accuracy of the model.\n",
        "- Handling high-dimensional data: Random Forest can effectively handle high-dimensional data by randomly selecting a subset of features at each node."
      ],
      "metadata": {
        "id": "fNMWm8ZQLEdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "ANS-> OOB (Out-of-Bag) score is a measure of the accuracy of a Random Forest model on unseen data. Here's a breakdown:\n",
        "\n",
        "What is OOB score?\n",
        "\n",
        "In Random Forest, each decision tree is trained on a bootstrap sample of the training data. The OOB score is calculated by evaluating the performance of each decision tree on the samples that were not used in its training (i.e., the \"out-of-bag\" samples).\n",
        "\n",
        "How is OOB score calculated?\n",
        "\n",
        "The OOB score is calculated as follows:\n",
        "\n",
        "1. Bootstrap sampling: A bootstrap sample of the training data is created for each decision tree.\n",
        "2. Decision tree training: Each decision tree is trained on its corresponding bootstrap sample.\n",
        "3. OOB prediction: Each decision tree makes predictions on the samples that were not used in its training (i.e., the OOB samples).\n",
        "4. OOB error calculation: The error of each decision tree on its OOB samples is calculated.\n",
        "5. OOB score calculation: The OOB score is calculated as the average error of all decision trees on their OOB samples."
      ],
      "metadata": {
        "id": "8p1Hgs_CLEg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "ANS-> Measuring the importance of features in a Random Forest model can be done using several methods. Here are some common approaches:\n",
        "\n",
        "1. Permutation Feature Importance: This method calculates the importance of each feature by measuring the decrease in model performance when the feature is randomly permuted. The idea is that if a feature is important, permuting its values should significantly decrease the model's performance.\n",
        "\n",
        "2. Gini Importance: This method calculates the importance of each feature by measuring the reduction in impurity (e.g., Gini impurity) when the feature is used to split the data. The idea is that features that result in a larger reduction in impurity are more important.\n",
        "\n",
        "3. Mean Decrease in Impurity (MDI): This method is similar to Gini importance but calculates the average reduction in impurity across all trees in the forest.\n",
        "\n",
        "4. Mean Decrease in Accuracy (MDA): This method calculates the importance of each feature by measuring the decrease in model accuracy when the feature is randomly permuted."
      ],
      "metadata": {
        "id": "agKj_vZaLEkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "ANS-> A Bagging Classifier, also known as Bootstrap Aggregating, is an ensemble learning algorithm that combines multiple instances of a base classifier to improve the accuracy and robustness of the model. Here's a step-by-step explanation of how a Bagging Classifier works:\n",
        "\n",
        "Step 1: Bootstrap Sampling\n",
        "\n",
        "The training dataset is sampled with replacement to create multiple bootstrap samples. Each bootstrap sample has the same size as the original training dataset.\n",
        "\n",
        "Step 2: Base Classifier Training\n",
        "\n",
        "A base classifier is trained on each bootstrap sample. The base classifier can be any type of classifier, such as a decision tree, logistic regression, or support vector machine.\n",
        "\n",
        "Step 3: Prediction\n",
        "\n",
        "Each trained base classifier makes predictions on the test dataset.\n",
        "\n",
        "Step 4: Voting\n",
        "\n",
        "The predictions from each base classifier are combined using voting. In classification problems, the class with the most votes is selected as the final prediction.\n",
        "\n",
        "Step 5: Final Prediction\n",
        "\n",
        "The final prediction is made by combining the predictions from all base classifiers.\n",
        "\n",
        "Key Benefits\n",
        "\n",
        "Bagging Classifiers offer several benefits:\n",
        "\n",
        "- Improved Accuracy: By combining multiple base classifiers, Bagging Classifiers can improve the overall accuracy of the model.\n",
        "- Reduced Overfitting: Bagging Classifiers can reduce overfitting by averaging the predictions from multiple base classifiers.\n",
        "- Increased Robustness: Bagging Classifiers can handle noisy data and outliers more effectively than a single base classifier."
      ],
      "metadata": {
        "id": "7VNPTH1FLEnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How do you evaluate a Bagging Classifierâ€™s performance?\n",
        "\n",
        "ANS-> Evaluating a Bagging Classifier's performance involves assessing its accuracy, robustness, and reliability. Here are some metrics and techniques to evaluate a Bagging Classifier's performance:\n",
        "\n",
        "Metrics:\n",
        "\n",
        "1. Accuracy: Measures the proportion of correctly classified instances.\n",
        "2. Precision: Measures the proportion of true positives among all positive predictions.\n",
        "3. Recall: Measures the proportion of true positives among all actual positive instances.\n",
        "4. F1-score: Measures the harmonic mean of precision and recall.\n",
        "5. Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values.\n",
        "6. Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual values.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "1. Cross-Validation: Splits the dataset into training and testing sets, and evaluates the model's performance on unseen data.\n",
        "2. Confusion Matrix: Provides a summary of predictions against actual outcomes, helping to identify errors and biases.\n",
        "3. Receiver Operating Characteristic (ROC) Curve: Plots the true positive rate against the false positive rate, helping to evaluate the model's performance at different thresholds.\n",
        "4. Precision-Recall Curve: Plots the precision against the recall, helping to evaluate the model's performance at different thresholds."
      ],
      "metadata": {
        "id": "ZZoWCd52LEp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. How does a Bagging Regressor work.\n",
        "\n",
        "ANS-> A Bagging Regressor, also known as Bootstrap Aggregating Regressor, is an ensemble learning algorithm that combines multiple instances of a base regressor to improve the accuracy and robustness of the model. Here's a step-by-step explanation of how a Bagging Regressor works:\n",
        "\n",
        "Step 1: Bootstrap Sampling\n",
        "\n",
        "The training dataset is sampled with replacement to create multiple bootstrap samples. Each bootstrap sample has the same size as the original training dataset.\n",
        "\n",
        "Step 2: Base Regressor Training\n",
        "\n",
        "A base regressor is trained on each bootstrap sample. The base regressor can be any type of regressor, such as a decision tree, linear regression, or support vector machine.\n",
        "\n",
        "Step 3: Prediction\n",
        "\n",
        "Each trained base regressor makes predictions on the test dataset.\n",
        "\n",
        "Step 4: Averaging\n",
        "\n",
        "The predictions from each base regressor are averaged to produce the final prediction.\n",
        "\n",
        "Key Benefits\n",
        "\n",
        "Bagging Regressors offer several benefits:\n",
        "\n",
        "- Improved Accuracy: By combining multiple base regressors, Bagging Regressors can improve the overall accuracy of the model.\n",
        "- Reduced Overfitting: Bagging Regressors can reduce overfitting by averaging the predictions from multiple base regressors.\n",
        "- Increased Robustness: Bagging Regressors can handle noisy data and outliers more effectively than a single base regressor."
      ],
      "metadata": {
        "id": "EPhxOGrALEtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "ANS-> The main advantage of ensemble techniques is that they can significantly improve the accuracy and robustness of machine learning models. Here are some specific benefits:\n",
        "\n",
        "1. Improved Accuracy: Ensemble techniques can combine the predictions of multiple models, reducing the impact of individual errors and resulting in more accurate predictions.\n",
        "2. Increased Robustness: Ensemble techniques can reduce the variance of predictions, making them more robust to changes in the data or model parameters.\n",
        "3. Better Handling of Overfitting: Ensemble techniques can reduce overfitting by averaging the predictions of multiple models, each of which may have overfit to different aspects of the data.\n",
        "4. Improved Handling of Noisy Data: Ensemble techniques can reduce the impact of noisy data by averaging the predictions of multiple models, each of which may have been affected by the noise in different ways.\n",
        "5. Flexibility: Ensemble techniques can be used with a wide range of machine learning models, including decision trees, neural networks, and support vector machines.\n",
        "\n",
        "Some popular ensemble techniques include:\n",
        "\n",
        "- Bagging: A technique that combines the predictions of multiple models trained on different subsets of the data.\n",
        "- Boosting: A technique that combines the predictions of multiple models trained on different subsets of the data, with each model attempting to correct the errors of the previous model.\n",
        "- Stacking: A technique that combines the predictions of multiple models using a meta-model that learns how to weight the predictions of each model."
      ],
      "metadata": {
        "id": "sypml5leLEwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the main challenge of ensemble methods?\n",
        "\n",
        "ANS-> The main challenges of ensemble methods are:\n",
        "\n",
        "1. Computational Cost: Ensemble methods can be computationally expensive, especially when dealing with large datasets or complex models.\n",
        "2. Model Selection: Choosing the right base models and hyperparameters for the ensemble can be challenging.\n",
        "3. Overfitting: Ensemble methods can suffer from overfitting, especially when the base models are complex or when the ensemble is not regularized properly.\n",
        "4. Interpretability: Ensemble methods can be difficult to interpret, especially when the base models are complex or when the ensemble is composed of many models.\n",
        "5. Hyperparameter Tuning: Ensemble methods often require tuning multiple hyperparameters, which can be time-consuming and challenging.\n",
        "\n",
        "Some specific challenges associated with popular ensemble methods are:\n",
        "\n",
        "- Bagging: Choosing the optimal number of bootstrap samples and the optimal size of each sample.\n",
        "- Boosting: Choosing the optimal learning rate and the optimal number of iterations.\n",
        "- Stacking: Choosing the optimal meta-model and the optimal way to combine the predictions of the base models.\n"
      ],
      "metadata": {
        "id": "9_Ec2UhYLEy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. Explain the key idea behind ensemble techniques.\n",
        "\n",
        "ANS-> The key idea behind ensemble techniques is to combine the predictions of multiple models to produce a more accurate and robust prediction than any individual model. This is based on the idea that different models can capture different patterns and relationships in the data, and that by combining their predictions, we can produce a more comprehensive and accurate prediction.\n",
        "\n",
        "There are several key concepts that underlie ensemble techniques:\n",
        "\n",
        "1. Diversity: Ensemble techniques rely on the idea that different models can capture different patterns and relationships in the data. This diversity can come from using different algorithms, different hyperparameters, or different subsets of the data.\n",
        "2. Combination: Ensemble techniques combine the predictions of multiple models to produce a single prediction. This can be done using techniques such as voting, averaging, or stacking.\n",
        "3. Improved Accuracy: By combining the predictions of multiple models, ensemble techniques can produce a more accurate prediction than any individual model.\n",
        "4. Robustness: Ensemble techniques can also improve the robustness of predictions by reducing the impact of individual model errors."
      ],
      "metadata": {
        "id": "GW9MaaYMdnHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. What is a Random Forest Classifier?\n",
        "\n",
        "ANS-> A Random Forest Classifier is a type of ensemble learning algorithm that combines multiple decision trees to classify data. It is a popular machine learning algorithm used for classification tasks.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "1. Decision Tree Creation: Multiple decision trees are created on random subsets of the training data.\n",
        "2. Random Feature Selection: At each node of the decision tree, a random subset of features is selected to consider for splitting.\n",
        "3. Voting: Each decision tree makes a prediction, and the final prediction is made by voting (i.e., selecting the class with the most votes).\n",
        "\n",
        "Key Benefits:\n",
        "\n",
        "1. Improved Accuracy: Random Forest Classifiers can achieve higher accuracy than individual decision trees.\n",
        "2. Reduced Overfitting: Random Forest Classifiers can reduce overfitting by averaging the predictions of multiple decision trees.\n",
        "3. Handling High-Dimensional Data: Random Forest Classifiers can handle high-dimensional data with a large number of features.\n",
        "4. Interpretable Results: Random Forest Classifiers provide feature importance scores, which can help interpret the results.\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "1. n_estimators: The number of decision trees in the forest.\n",
        "2. max_depth: The maximum depth of each decision tree.\n",
        "3. min_samples_split: The minimum number of samples required to split an internal node.\n",
        "4. min_samples_leaf: The minimum number of samples required to be at a leaf node."
      ],
      "metadata": {
        "id": "BHQCpOH9dnRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What are the main types of ensemble techniques?\n",
        "\n",
        "ANS-> There are several types of ensemble techniques, including:\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating): Involves training multiple models on different subsets of the training data and combining their predictions.\n",
        "2. Boosting: Involves training multiple models sequentially, with each model attempting to correct the errors of the previous model.\n",
        "3. Stacking: Involves training multiple models and combining their predictions using a meta-model.\n",
        "4. Random Forests: Involves training multiple decision trees on random subsets of the training data and combining their predictions.\n",
        "5. Gradient Boosting: A type of boosting that uses gradient descent to optimize the weights of the models.\n",
        "6. AdaBoost: A type of boosting that adaptively adjusts the weights of the models based on their performance.\n",
        "7. Gradient Boosting Machines (GBMs): A type of gradient boosting that uses decision trees as the base models.\n",
        "8. Extreme Gradient Boosting (XGBoost): A type of gradient boosting that uses decision trees as the base models and is optimized for performance and speed.\n",
        "\n",
        "Each of these ensemble techniques has its own strengths and weaknesses, and the choice of which one to use depends on the specific problem and dataset.\n",
        "\n",
        "Bagging and Boosting are the two most fundamental types of ensemble techniques.\n",
        "\n",
        "- Bagging is used for reducing variance, by averaging the predictions of multiple models.\n",
        "- Boosting is used for reducing bias, by sequentially training models to correct the errors of the previous model."
      ],
      "metadata": {
        "id": "pp1Ry5v4dnU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. What is ensemble learning in machine learning?\n",
        "\n",
        "ANS-> Ensemble learning is a machine learning technique that combines the predictions of multiple models to improve the accuracy and robustness of the predictions. The idea is to train multiple models on the same data and then combine their predictions to produce a final output.\n",
        "\n",
        "Why Ensemble Learning?\n",
        "\n",
        "Ensemble learning is useful for several reasons:\n",
        "\n",
        "1. Improved Accuracy: By combining the predictions of multiple models, ensemble learning can often achieve better accuracy than any individual model.\n",
        "2. Reduced Overfitting: Ensemble learning can help reduce overfitting by averaging the predictions of multiple models, which can help cancel out errors.\n",
        "3. Increased Robustness: Ensemble learning can make the model more robust to changes in the data or to noisy data.\n",
        "\n",
        "Types of Ensemble Learning\n",
        "\n",
        "There are several types of ensemble learning techniques:\n",
        "\n",
        "1. Bagging: Involves training multiple models on different subsets of the data and combining their predictions.\n",
        "2. Boosting: Involves training multiple models sequentially, with each model attempting to correct the errors of the previous model.\n",
        "3. Stacking: Involves training multiple models and combining their predictions using a meta-model.\n",
        "\n",
        "How Ensemble Learning Works\n",
        "\n",
        "The process of ensemble learning typically involves the following steps:\n",
        "\n",
        "1. Train Multiple Models: Train multiple models on the same data.\n",
        "2. Make Predictions: Make predictions using each model.\n",
        "3. Combine Predictions: Combine the predictions of each model to produce a final output."
      ],
      "metadata": {
        "id": "aw9rS0kIdnYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. When should we avoid using ensemble methods?\n",
        "\n",
        "ANS-> While ensemble methods can be powerful tools for improving the accuracy and robustness of machine learning models, there are certain situations where they may not be the best choice. Here are some scenarios where you might want to avoid using ensemble methods:\n",
        "\n",
        "1. Small Datasets: Ensemble methods typically require large amounts of data to be effective. If you have a small dataset, you may not have enough data to train multiple models, and ensemble methods may not provide any benefits.\n",
        "2. Simple Problems: If you have a simple problem that can be solved with a single model, there may be no need to use ensemble methods. Ensemble methods can add complexity to your model, and may not provide any benefits for simple problems.\n",
        "3. Interpretable Models: If you need to interpret the results of your model, ensemble methods may not be the best choice. Ensemble methods can make it difficult to understand how the model is making predictions, as the predictions are based on the combined output of multiple models.\n",
        "4. Real-Time Systems: Ensemble methods can be computationally expensive, which can make them difficult to use in real-time systems. If you need to make predictions in real-time, you may want to consider using a single model instead of an ensemble.\n",
        "5. Overfitting: Ensemble methods can sometimes overfit the data, especially if the individual models are complex. If you are working with a small dataset, you may want to avoid using ensemble methods to prevent overfitting.\n",
        "6. Class Imbalance: Ensemble methods can sometimes exacerbate class imbalance problems. If you have a dataset with a large class imbalance, you may want to consider using techniques such as oversampling the minority class, undersampling the majority class, or using class weights instead of ensemble methods.\n",
        "7. High-Dimensional Data: Ensemble methods can be difficult to apply to high-dimensional data, as the number of features can make it difficult to train multiple models. In these cases, you may want to consider using dimensionality reduction techniques instead of ensemble methods.\n"
      ],
      "metadata": {
        "id": "dj7w0n5ydnco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "ANS-> Bagging (Bootstrap Aggregating) is an ensemble learning technique that helps reduce overfitting in several ways:\n",
        "\n",
        "1. Averaging Multiple Models: Bagging combines the predictions of multiple models trained on different subsets of the data. By averaging the predictions, Bagging reduces the impact of individual model errors, which can help reduce overfitting.\n",
        "2. Reducing Variance: Bagging reduces the variance of the model by averaging the predictions of multiple models. When individual models have high variance, they may overfit the training data. By averaging the predictions, Bagging reduces the overall variance of the model.\n",
        "3. Increasing Model Stability: Bagging increases the stability of the model by reducing the impact of individual data points. When a single data point has a large impact on the model, it can cause overfitting. By averaging the predictions of multiple models, Bagging reduces the impact of individual data points.\n",
        "4. Reducing Model Complexity: Bagging can reduce the complexity of the model by using simpler base models. When individual models are complex, they may overfit the training data. By using simpler base models, Bagging reduces the overall complexity of the model.\n",
        "\n",
        "How Bagging Reduces Overfitting\n",
        "\n",
        "To illustrate how Bagging reduces overfitting, consider the following example:\n",
        "\n",
        "Suppose we have a dataset with 100 samples, and we want to train a decision tree model. Without Bagging, the decision tree model may overfit the training data, especially if the tree is deep.\n",
        "\n",
        "With Bagging, we create multiple subsets of the data, each with 100 samples (with replacement). We then train a decision tree model on each subset. The predictions from each model are then averaged to produce the final prediction.\n",
        "\n",
        "By averaging the predictions of multiple models, Bagging reduces the impact of individual model errors, which can help reduce overfitting. Additionally, Bagging reduces the variance of the model, increases model stability, and reduces model complexity, all of which can contribute to reducing overfitting."
      ],
      "metadata": {
        "id": "79GopJqhdngW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "ANS-> Random Forest is generally better than a single Decision Tree for several reasons:\n",
        "\n",
        "1. Reduced Overfitting: Random Forest reduces overfitting by averaging the predictions of multiple decision trees. Each decision tree in the forest is trained on a random subset of the data, which helps to reduce overfitting.\n",
        "2. Improved Accuracy: Random Forest can improve the accuracy of predictions by combining the predictions of multiple decision trees. Each decision tree may capture different patterns in the data, and by combining their predictions, Random Forest can produce more accurate results.\n",
        "3. Increased Robustness: Random Forest is more robust than a single decision tree because it can handle missing values and outliers more effectively. If one decision tree in the forest is affected by missing values or outliers, the other trees can still produce accurate predictions.\n",
        "4. Better Handling of High-Dimensional Data: Random Forest can handle high-dimensional data more effectively than a single decision tree. By selecting a random subset of features at each node, Random Forest can reduce the dimensionality of the data and improve the accuracy of predictions.\n",
        "5. Parallelization: Random Forest can be parallelized more easily than a single decision tree. By training multiple decision trees in parallel, Random Forest can take advantage of multi-core processors and produce results more quickly.\n",
        "\n",
        "Why Single Decision Trees Can Be Problematic\n",
        "\n",
        "Single decision trees can be problematic for several reasons:\n",
        "\n",
        "1. Overfitting: Decision trees can overfit the training data, especially if the tree is deep or if the data is noisy.\n",
        "2. High Variance: Decision trees can have high variance, which means that small changes in the training data can result in large changes in the predictions.\n",
        "3. Sensitive to Hyperparameters: Decision trees can be sensitive to hyperparameters, such as the maximum depth of the tree or the minimum number of samples required at each node."
      ],
      "metadata": {
        "id": "_kVFcn4adnj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "ANS-> Bootstrap sampling plays a crucial role in Bagging (Bootstrap Aggregating) by allowing the algorithm to create multiple versions of the training dataset, which in turn enables the creation of multiple models. Here's how bootstrap sampling contributes to Bagging:\n",
        "\n",
        "What is Bootstrap Sampling?\n",
        "\n",
        "Bootstrap sampling is a resampling technique that involves creating a new dataset by randomly selecting samples from the original dataset with replacement. This means that some samples may be selected multiple times, while others may not be selected at all.\n",
        "\n",
        "Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "In Bagging, bootstrap sampling is used to create multiple versions of the training dataset. Here's how it works:\n",
        "\n",
        "1. Create Bootstrap Samples: Create B bootstrap samples from the original training dataset. Each bootstrap sample is created by randomly selecting n samples from the original dataset with replacement, where n is the size of the original dataset.\n",
        "2. Train a Model on Each Bootstrap Sample: Train a model on each of the B bootstrap samples. This results in B different models, each trained on a slightly different version of the training dataset.\n",
        "3. Combine the Predictions: Combine the predictions of the B models to produce the final prediction.\n",
        "\n",
        "Benefits of Bootstrap Sampling in Bagging\n",
        "\n",
        "The use of bootstrap sampling in Bagging provides several benefits:\n",
        "\n",
        "1. Reduces Overfitting: By creating multiple versions of the training dataset, Bagging reduces the risk of overfitting to a single dataset.\n",
        "2. Improves Model Robustness: By training multiple models on different versions of the training dataset, Bagging improves the robustness of the final model to changes in the data.\n",
        "3. Increases Model Diversity: By creating multiple models, each trained on a slightly different version of the training dataset, Bagging increases the diversity of the models, which can lead to better performance."
      ],
      "metadata": {
        "id": "9ZHM_RbVdnnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "ANS-> Ensemble techniques have a wide range of real-world applications across various industries. Here are some examples:\n",
        "\n",
        "1. Credit Risk Assessment: Ensemble methods are used in credit risk assessment to predict the likelihood of loan defaults. By combining the predictions of multiple models, lenders can make more accurate decisions about loan approvals.\n",
        "2. Image Classification: Ensemble techniques are used in image classification tasks such as object detection, facial recognition, and image segmentation. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of image classification tasks.\n",
        "3. Natural Language Processing (NLP): Ensemble methods are used in NLP tasks such as sentiment analysis, text classification, and language translation. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of NLP tasks.\n",
        "4. Recommendation Systems: Ensemble techniques are used in recommendation systems to predict user preferences and recommend products or services. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of recommendation systems.\n",
        "5. Medical Diagnosis: Ensemble methods are used in medical diagnosis to predict the likelihood of diseases such as cancer, diabetes, and cardiovascular disease. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of medical diagnosis.\n",
        "6. Financial Forecasting: Ensemble techniques are used in financial forecasting to predict stock prices, currency exchange rates, and commodity prices. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of financial forecasting.\n",
        "7. Customer Churn Prediction: Ensemble methods are used in customer churn prediction to predict the likelihood of customers switching to a competitor. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of customer churn prediction.\n",
        "8. Speech Recognition: Ensemble techniques are used in speech recognition to improve the accuracy of speech recognition systems. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of speech recognition.\n",
        "9. Time Series Forecasting: Ensemble methods are used in time series forecasting to predict future values of a time series. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of time series forecasting.\n",
        "10. Anomaly Detection: Ensemble techniques are used in anomaly detection to identify unusual patterns or outliers in data. By combining the predictions of multiple models, ensemble techniques can improve the accuracy of anomaly detection."
      ],
      "metadata": {
        "id": "n_Xxq2Khdnqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "ANS-> Bagging (Bootstrap Aggregating) and Boosting are two popular ensemble learning techniques used to improve the performance of machine learning models. While both techniques involve combining multiple models to produce a more accurate prediction, there are key differences between them:\n",
        "\n",
        "Bagging:\n",
        "\n",
        "1. Parallel Training: In Bagging, multiple models are trained in parallel on different subsets of the training data.\n",
        "2. Random Sampling: Each model is trained on a random subset of the training data, with replacement.\n",
        "3. Equal Weighting: Each model is given equal weight when combining their predictions.\n",
        "4. Reduces Variance: Bagging reduces the variance of the model by averaging the predictions of multiple models.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "1. Sequential Training: In Boosting, multiple models are trained sequentially, with each model attempting to correct the errors of the previous model.\n",
        "2. Weighted Sampling: Each model is trained on a weighted subset of the training data, where the weights are adjusted based on the performance of the previous model.\n",
        "3. Weighted Combining: Each model is given a weighted vote when combining their predictions, with the weights determined by the model's performance.\n",
        "4. Reduces Bias: Boosting reduces the bias of the model by sequentially training models to correct the errors of the previous model.\n"
      ],
      "metadata": {
        "id": "PZfxPN-_dnuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "P7m4SHhgdnxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?"
      ],
      "metadata": {
        "id": "8BAclYmmfp0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset (replace with your actual data)\n",
        "data = {'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
        "        'target': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimators\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8xSOhNafqEP",
        "outputId": "4d2010c9-d9f5-40f0-f5e0-eedfe4e0c3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "ThXV70sefqOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset (replace with your actual data)\n",
        "# Create a sample dataset with a numerical target variable\n",
        "np.random.seed(0)\n",
        "n_samples = 100\n",
        "X = np.random.rand(n_samples, 2)  # Two features\n",
        "y = 2*X[:, 0] + 3*X[:, 1] + np.random.randn(n_samples)  # Target with some noise\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees as base estimators\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOS0gEpQfqW9",
        "outputId": "562039ee-eb05-49fd-807d-a564216d3b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.9141228812740874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "fen-Cto1fqek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores2\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Print feature importance scores\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jIf95_kfqod",
        "outputId": "9bde155e-6a0d-41c9-fac6-f9bfbab0f434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: 0.048703371737755234\n",
            "Feature 1: 0.013590877656998469\n",
            "Feature 2: 0.053269746128179675\n",
            "Feature 3: 0.04755500886018552\n",
            "Feature 4: 0.007285327830663239\n",
            "Feature 5: 0.013944325074050485\n",
            "Feature 6: 0.06800084191430111\n",
            "Feature 7: 0.10620998844591638\n",
            "Feature 8: 0.003770291819290666\n",
            "Feature 9: 0.0038857721093275\n",
            "Feature 10: 0.02013891719419153\n",
            "Feature 11: 0.004723988073894702\n",
            "Feature 12: 0.01130301388178435\n",
            "Feature 13: 0.022406960160458473\n",
            "Feature 14: 0.004270910110504497\n",
            "Feature 15: 0.005253215538990106\n",
            "Feature 16: 0.009385832251596627\n",
            "Feature 17: 0.003513255105598506\n",
            "Feature 18: 0.004018418617722808\n",
            "Feature 19: 0.00532145634222884\n",
            "Feature 20: 0.07798687515738047\n",
            "Feature 21: 0.021749011006763207\n",
            "Feature 22: 0.06711483267839194\n",
            "Feature 23: 0.15389236463205394\n",
            "Feature 24: 0.010644205147280952\n",
            "Feature 25: 0.020266035899623565\n",
            "Feature 26: 0.031801595740040434\n",
            "Feature 27: 0.14466326620735528\n",
            "Feature 28: 0.010120176131974357\n",
            "Feature 29: 0.005210118545497296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24. Train a Random Forest Regressor and compare its performance with a single Decision Tree."
      ],
      "metadata": {
        "id": "dfLBIWtafqv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset (replace with your actual data)\n",
        "np.random.seed(0)\n",
        "n_samples = 100\n",
        "X = np.random.rand(n_samples, 2)  # Two features\n",
        "y = 2*X[:, 0] + 3*X[:, 1] + np.random.randn(n_samples)  # Target with some noise\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "rf_predictions = rf_regressor.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "print(f\"Random Forest MSE: {rf_mse}\")\n",
        "\n",
        "# Single Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "dt_predictions = dt_regressor.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
        "print(f\"Decision Tree MSE: {dt_mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mha2hv4fq4j",
        "outputId": "43cef317-2d36-41ed-ff7a-11113883329f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest MSE: 0.9720851384756909\n",
            "Decision Tree MSE: 1.9979423446369762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."
      ],
      "metadata": {
        "id": "rjSJyJPbfrvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with OOB scoring enabled\n",
        "rf_classifier = RandomForestClassifier(random_state=42, oob_score=True)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get the OOB score\n",
        "oob_score = rf_classifier.oob_score_\n",
        "\n",
        "# Print the OOB score\n",
        "print(f\"Out-of-Bag Score: {oob_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnFNuZQOfr6N",
        "outputId": "e9dcc3f5-0c50-47d1-ef8b-9b2da977df2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag Score: 0.9560439560439561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26. Train a Bagging Classifier using SVM as a base estimator and print accuracy."
      ],
      "metadata": {
        "id": "pkabvLeEfsGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset (replace with your actual data)\n",
        "# Increased the dataset size and introduced more variability in the target\n",
        "data = {'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "        'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 11, 12, 13, 14, 15],\n",
        "        'target': [0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as the base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNYud0lHg3I-",
        "outputId": "8a048942-2305-454e-ff8d-e97511b343e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27. Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ],
      "metadata": {
        "id": "H6bVqWetfsIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Number of trees to try\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    # Create a Random Forest Classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "    # Train the classifier\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate and print the accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of trees: {n_estimators}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2gtqp5MfsTI",
        "outputId": "31b4c5c7-c8c4-4e28-962e-06b0a2a7d84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trees: 10, Accuracy: 0.956140350877193\n",
            "Number of trees: 50, Accuracy: 0.9649122807017544\n",
            "Number of trees: 100, Accuracy: 0.9649122807017544\n",
            "Number of trees: 200, Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
      ],
      "metadata": {
        "id": "KsQYQMQgfsfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Sample dataset (replace with your actual data)\n",
        "# Increased the dataset size and introduced more variability in the target\n",
        "data = {'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "        'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 11, 12, 13, 14, 15],\n",
        "        'target': [0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as the base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=LogisticRegression(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_proba = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print the AUC score\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"AUC Score: {auc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw1Eu96DfsqF",
        "outputId": "684c6ffe-5574-4cef-d1ee-54b362e57c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29. Train a Random Forest Regressor and analyze feature importance scores."
      ],
      "metadata": {
        "id": "c-vAfWqsfs0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Regressor and analyze feature importance scores\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a sample dataset (replace with your actual data)\n",
        "X, y = make_regression(n_samples=100, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_regressor.feature_importances_\n",
        "\n",
        "# Analyze feature importance scores\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i}: {importance}\")\n",
        "\n",
        "# You can further analyze the feature importances by:\n",
        "# 1. Sorting them in descending order to identify the most important features.\n",
        "# 2. Visualizing them using a bar chart or other suitable plot.\n",
        "# 3. Using the feature importances to perform feature selection.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq3oeQzHftCF",
        "outputId": "541dcd49-6ae1-4188-dd1d-22f72cddad6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: 0.013544815404741738\n",
            "Feature 1: 0.43365139336304465\n",
            "Feature 2: 0.46323172285772857\n",
            "Feature 3: 0.014030728474509637\n",
            "Feature 4: 0.019138850710003583\n",
            "Feature 5: 0.012024905691070419\n",
            "Feature 6: 0.012238615146632176\n",
            "Feature 7: 0.011283795135293878\n",
            "Feature 8: 0.013120994207303595\n",
            "Feature 9: 0.007734179009671741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "ymEUfDC3ftMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_predictions = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5WVlv_AftXW",
        "outputId": "4085deeb-ebf9-4b63-c581-d410ea221670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.956140350877193\n",
            "Random Forest Classifier Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
      ],
      "metadata": {
        "id": "guzVhmp8hi0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding accuracy score\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy Score:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "test_accuracy = best_rf_classifier.score(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N8jvw4hhi8N",
        "outputId": "3183ab18-25e8-45b7-a3fc-8ae894aa1229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Best Accuracy Score: 0.9626373626373625\n",
            "Test Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q32. Train a Bagging Regressor with different numbers of base estimators and compare performance."
      ],
      "metadata": {
        "id": "0Pm0U-iZhjDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Import make_regression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=100, n_features=10, n_informative=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Number of estimators to try\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    # Create and train a Bagging Regressor\n",
        "    bagging_regressor = BaggingRegressor(n_estimators=n_estimators, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and evaluate performance\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Number of estimators: {n_estimators}, Mean Squared Error: {mse}\")"
      ],
      "metadata": {
        "id": "Dr4Cn5BhhjO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792c0ac9-dab1-47d7-9d18-b63e5e7c472d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of estimators: 10, Mean Squared Error: 2560.457811489844\n",
            "Number of estimators: 50, Mean Squared Error: 2246.8589838983935\n",
            "Number of estimators: 100, Mean Squared Error: 2227.886257266598\n",
            "Number of estimators: 200, Mean Squared Error: 2312.0285304028166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q33. Train a Random Forest Classifier and analyze misclassified samples."
      ],
      "metadata": {
        "id": "UIce5lN-hjWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_indices = y_test != y_pred\n",
        "misclassified_samples = X_test[misclassified_indices]\n",
        "misclassified_true_labels = y_test[misclassified_indices]\n",
        "misclassified_predicted_labels = y_pred[misclassified_indices]\n",
        "\n",
        "# Print or further analyze the misclassified samples\n",
        "print(\"Misclassified Samples:\")\n",
        "for i in range(len(misclassified_samples)):\n",
        "    print(f\"Sample {i + 1}:\")\n",
        "    print(f\"  Features: {misclassified_samples[i]}\")\n",
        "    # Accessing elements of NumPy array directly using index\n",
        "    print(f\"  True Label: {misclassified_true_labels[i]}\")\n",
        "    print(f\"  Predicted Label: {misclassified_predicted_labels[i]}\")"
      ],
      "metadata": {
        "id": "6r-0-r_Dhjfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86386649-0d18-4490-f586-82e898b0ab17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n",
            "Misclassified Samples:\n",
            "Sample 1:\n",
            "  Features: [1.334e+01 1.586e+01 8.649e+01 5.200e+02 1.078e-01 1.535e-01 1.169e-01\n",
            " 6.987e-02 1.942e-01 6.902e-02 2.860e-01 1.016e+00 1.535e+00 1.296e+01\n",
            " 6.794e-03 3.575e-02 3.980e-02 1.383e-02 2.134e-02 4.603e-03 1.553e+01\n",
            " 2.319e+01 9.666e+01 6.149e+02 1.536e-01 4.791e-01 4.858e-01 1.708e-01\n",
            " 3.527e-01 1.016e-01]\n",
            "  True Label: 1\n",
            "  Predicted Label: 0\n",
            "Sample 2:\n",
            "  Features: [1.380e+01 1.579e+01 9.043e+01 5.841e+02 1.007e-01 1.280e-01 7.789e-02\n",
            " 5.069e-02 1.662e-01 6.566e-02 2.787e-01 6.205e-01 1.957e+00 2.335e+01\n",
            " 4.717e-03 2.065e-02 1.759e-02 9.206e-03 1.220e-02 3.130e-03 1.657e+01\n",
            " 2.086e+01 1.103e+02 8.124e+02 1.411e-01 3.542e-01 2.779e-01 1.383e-01\n",
            " 2.589e-01 1.030e-01]\n",
            "  True Label: 0\n",
            "  Predicted Label: 1\n",
            "Sample 3:\n",
            "  Features: [1.396e+01 1.705e+01 9.143e+01 6.024e+02 1.096e-01 1.279e-01 9.789e-02\n",
            " 5.246e-02 1.908e-01 6.130e-02 4.250e-01 8.098e-01 2.563e+00 3.574e+01\n",
            " 6.351e-03 2.679e-02 3.119e-02 1.342e-02 2.062e-02 2.695e-03 1.639e+01\n",
            " 2.207e+01 1.081e+02 8.260e+02 1.512e-01 3.262e-01 3.209e-01 1.374e-01\n",
            " 3.068e-01 7.957e-02]\n",
            "  True Label: 0\n",
            "  Predicted Label: 1\n",
            "Sample 4:\n",
            "  Features: [1.448e+01 2.146e+01 9.425e+01 6.482e+02 9.444e-02 9.947e-02 1.204e-01\n",
            " 4.938e-02 2.075e-01 5.636e-02 4.204e-01 2.220e+00 3.301e+00 3.887e+01\n",
            " 9.369e-03 2.983e-02 5.371e-02 1.761e-02 2.418e-02 3.249e-03 1.621e+01\n",
            " 2.925e+01 1.084e+02 8.089e+02 1.306e-01 1.976e-01 3.349e-01 1.225e-01\n",
            " 3.020e-01 6.846e-02]\n",
            "  True Label: 0\n",
            "  Predicted Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
      ],
      "metadata": {
        "id": "CXIeB4chhjpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier # import the BaggingClassifier\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Sample dataset (replace with your actual data)\n",
        "data = {'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
        "        'target': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimators\n",
        "# Changed 'base_estimator' to 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "_kjrN5-_hjxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d30e5e76-93a3-4384-edb3-3534f42b9bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q35. Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "l_qgmNLvhj5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier # Import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer # Import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hVK_PnohhkC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "a70c6ad5-b983-45ca-ef0e-5542433cf64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
      ],
      "metadata": {
        "id": "BsfHDWGfhkNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base models\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)\n",
        "lr_clf = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=[('dt', dt_clf), ('svm', svm_clf), ('lr', lr_clf)], final_estimator=LogisticRegression(max_iter=1000))\n",
        "\n",
        "# Train the models\n",
        "dt_clf.fit(X_train, y_train)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "svm_pred = svm_clf.predict(X_test)\n",
        "lr_pred = lr_clf.predict(X_test)\n",
        "stacking_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
        "print(\"Stacking Classifier Accuracy:\", stacking_accuracy)"
      ],
      "metadata": {
        "id": "6dIh_4_qhkWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc8e9a6-10df-41ac-d8ed-9f9084f0b289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "SVM Accuracy: 1.0\n",
            "Logistic Regression Accuracy: 1.0\n",
            "Stacking Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q37. Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "HLXXlXQnhkfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Train a Random Forest Classifier and print the top 5 most important features\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Get the indices of the top 5 most important features\n",
        "top_5_indices = feature_importances.argsort()[-5:][::-1]\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 most important features:\")\n",
        "for i in top_5_indices:\n",
        "    print(f\"{data.feature_names[i]}: {feature_importances[i]}\")\n"
      ],
      "metadata": {
        "id": "q0DYfT0Phknu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6c8828-11e4-4170-e3a2-f75877af3d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "worst area: 0.15389236463205394\n",
            "worst concave points: 0.14466326620735528\n",
            "mean concave points: 0.10620998844591638\n",
            "worst radius: 0.07798687515738047\n",
            "mean concavity: 0.06800084191430111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "lFgjNlDthkxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score=\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimators\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ],
      "metadata": {
        "id": "Jqf-ZnVOhk5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85bda9c-5bb3-4a5d-c451-beb14d1c9234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9583333333333334\n",
            "Recall: 0.971830985915493\n",
            "F1-score: 0.965034965034965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "VneboDO_hlBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Max depths to try\n",
        "max_depths = [None, 5, 10, 15, 20]\n",
        "accuracy_scores = []\n",
        "\n",
        "for max_depth in max_depths:\n",
        "    # Create a Random Forest Classifier with the current max_depth\n",
        "    rf_classifier = RandomForestClassifier(max_depth=max_depth, random_state=42)\n",
        "\n",
        "    # Train the classifier\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    print(f\"Max depth: {max_depth}, Accuracy: {accuracy}\")\n",
        "\n",
        "# Plot the results (optional)\n",
        "plt.plot(max_depths, accuracy_scores, marker='o')\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Max Depth on Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4V64USobhlJ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "6edfca02-6177-4fb2-b1a6-fcf1152f5130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max depth: None, Accuracy: 0.9649122807017544\n",
            "Max depth: 5, Accuracy: 0.9649122807017544\n",
            "Max depth: 10, Accuracy: 0.9649122807017544\n",
            "Max depth: 15, Accuracy: 0.9649122807017544\n",
            "Max depth: 20, Accuracy: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance."
      ],
      "metadata": {
        "id": "Zr1vRwTthlcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "# performance\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=100, n_features=10, n_informative=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Regressor with Decision Tree as base estimator\n",
        "bagging_dt_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
        "bagging_dt_regressor.fit(X_train, y_train)\n",
        "y_pred_dt = bagging_dt_regressor.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "print(f\"Bagging Regressor (Decision Tree) MSE: {mse_dt}\")\n",
        "\n",
        "# Bagging Regressor with KNeighbors as base estimator\n",
        "bagging_knn_regressor = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=100, random_state=42)\n",
        "bagging_knn_regressor.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn_regressor.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "print(f\"Bagging Regressor (KNeighbors) MSE: {mse_knn}\")\n"
      ],
      "metadata": {
        "id": "B0QnYcrphllV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ed2a91-0f44-47b9-9084-04467fc13917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor (Decision Tree) MSE: 2227.886257266598\n",
            "Bagging Regressor (KNeighbors) MSE: 5050.23729553522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "uiEonsSVhlt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")\n"
      ],
      "metadata": {
        "id": "s5SGAEU8hl4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0762ff2-acef-4014-8c6f-1a9e61f49a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9952505732066819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q42. Train a Bagging Classifier and evaluate its performance using cross-validation."
      ],
      "metadata": {
        "id": "L8Qe7WdEDpR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Assuming X and y are already defined from the previous code\n",
        "\n",
        "# Create a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy') # 5-fold cross validation\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "\n",
        "# Print the mean and standard deviation of the scores\n",
        "print(f\"Mean accuracy: {cv_scores.mean()}\")\n",
        "print(f\"Standard deviation: {cv_scores.std()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fqWivhkDpbo",
        "outputId": "155a8887-ee8e-4f7c-be17-423bfc04e4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.92982456 0.93859649 0.97368421 0.93859649 0.97345133]\n",
            "Mean accuracy: 0.9508306163639186\n",
            "Standard deviation: 0.01883924263933025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q43. Train a Random Forest Classifier and plot the Precision-Recall curve."
      ],
      "metadata": {
        "id": "AHaQu3PaDpnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming rf_classifier and X_test, y_test are already defined from previous code\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall pairs for different probability thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Calculate the area under the precision-recall curve (AUC)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot the precision-recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "8NbgRmJADpvg",
        "outputId": "98ec7f5c-8663-43d9-fb7d-71c02eaad6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
      ],
      "metadata": {
        "id": "5ZXi4R_NDp5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base models\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_clf = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=[('rf', rf_clf), ('lr', lr_clf)], final_estimator=LogisticRegression(max_iter=1000))\n",
        "\n",
        "# Train the models\n",
        "rf_clf.fit(X_train, y_train)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "lr_pred = lr_clf.predict(X_test)\n",
        "stacking_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
        "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
        "print(\"Stacking Classifier Accuracy:\", stacking_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiqAoJU4DqCg",
        "outputId": "c9322811-d345-49de-f18c-363c22939e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 1.0\n",
            "Logistic Regression Accuracy: 1.0\n",
            "Stacking Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "dxDsLsVZDqKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=100, n_features=10, n_informative=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bootstrap sample ratios to try\n",
        "bootstrap_ratios = [0.5, 0.7, 0.9, 1.0]\n",
        "\n",
        "for ratio in bootstrap_ratios:\n",
        "    # Create and train a Bagging Regressor\n",
        "    bagging_regressor = BaggingRegressor(n_estimators=100, max_samples=ratio, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and evaluate performance\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bootstrap ratio: {ratio}, Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKERQF8QDqUx",
        "outputId": "0323ba45-a201-4947-b83b-dc04b2bc21de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap ratio: 0.5, Mean Squared Error: 2703.3028426828046\n",
            "Bootstrap ratio: 0.7, Mean Squared Error: 2197.269999873991\n",
            "Bootstrap ratio: 0.9, Mean Squared Error: 2119.600573032166\n",
            "Bootstrap ratio: 1.0, Mean Squared Error: 2227.886257266598\n"
          ]
        }
      ]
    }
  ]
}
