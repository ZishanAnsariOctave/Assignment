{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM19b2B4z6KpyviZa0tbDmQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HEMALATHA-jpg/function/blob/main/Hema_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical"
      ],
      "metadata": {
        "id": "yWzcb9DT0rXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "\n",
        "ANS-> ogistic Regression and Linear Regression are two fundamental algorithms in Machine Learning, used for predicting continuous and categorical outcomes, respectively.\n",
        "\n",
        "Linear Regression:\n",
        "\n",
        "Linear Regression is a supervised learning algorithm used for predicting continuous outcomes, such as stock prices, temperatures, or salaries. It models the relationship between a dependent variable (target) and one or more independent variables (features) using a linear equation.\n",
        "\n",
        "The goal of Linear Regression is to find the best-fitting line that minimizes the difference between predicted and actual values.\n",
        "\n",
        "Logistic Regression:\n",
        "\n",
        "Logistic Regression, also known as Logit Regression, is a supervised learning algorithm used for predicting categorical outcomes, such as 0/1, yes/no, or win/lose. It models the probability of an event occurring based on one or more independent variables.\n",
        "\n",
        "The goal of Logistic Regression is to find the best-fitting curve that separates the classes in the feature space.\n",
        "\n",
        "Key differences:\n",
        "\n",
        "1. Target variable: Linear Regression predicts continuous outcomes, while Logistic Regression predicts categorical outcomes.\n",
        "2. Modeling approach: Linear Regression uses a linear equation, while Logistic Regression uses a logistic function (S-curve) to model the probability of an event.\n",
        "3. Output interpretation: Linear Regression outputs a continuous value, while Logistic Regression outputs a probability between 0 and 1.\n",
        "4. Cost function: Linear Regression uses Mean Squared Error (MSE) or Mean Absolute Error (MAE), while Logistic Regression uses Log Loss (Cross-Entropy Loss)."
      ],
      "metadata": {
        "id": "m_ndTGCw0rjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the mathematical equation of Logistic Regression.\n",
        "\n",
        "ANS-> The mathematical equation for Logistic Regression is:\n",
        "\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "where:\n",
        "\n",
        "p = probability of the positive class (e.g., 1, yes, win)\n",
        "e = base of the natural logarithm (approximately 2.718)\n",
        "z = weighted sum of the input features\n",
        "\n",
        "The weighted sum z is calculated as:\n",
        "\n",
        "z = w0 + w1_x1 + w2_x2 + … + wn*xn\n",
        "\n",
        "where:\n",
        "\n",
        "w0 = bias term\n",
        "wi = weights for each feature\n",
        "xi = input features\n",
        "\n",
        "The logistic function, also known as the sigmoid function, maps the weighted sum z to a probability p between 0 and 1.\n",
        "\n",
        "This equation is used to predict the probability of the positive class, and the predicted class label is typically determined by thresholding the predicted probability (e.g., if p > 0.5, predict 1, otherwise predict 0)."
      ],
      "metadata": {
        "id": "Xli9ZSh90rmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "\n",
        "ANS-> The Sigmoid function, also known as the logistic function, is used in Logistic Regression for several reasons:\n",
        "\n",
        "1. Maps to probabilities: The Sigmoid function maps any real-valued number to a value between 0 and 1, which is ideal for representing probabilities.\n",
        "\n",
        "2. Non-linearity: The Sigmoid function introduces non-linearity to the model, allowing it to learn complex relationships between the features and the target variable.\n",
        "\n",
        "3. Easy to compute: The Sigmoid function is computationally efficient and easy to implement.\n",
        "\n",
        "4. Differentiable: The Sigmoid function is differentiable, which is necessary for computing gradients during backpropagation.\n",
        "\n",
        "5. Interpretability: The output of the Sigmoid function can be interpreted as a probability, making it easy to understand and communicate the results.\n",
        "\n",
        "Mathematically, the Sigmoid function has the following properties:\n",
        "\n",
        "- Sigmoid(−∞) = 0\n",
        "- Sigmoid(0) = 0.5\n",
        "- Sigmoid(∞) = 1\n"
      ],
      "metadata": {
        "id": "2wGPHBcN0rp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the cost function of Logistic Regression.\n",
        "\n",
        "ANS-> The cost function of Logistic Regression is called Log Loss, also known as Cross-Entropy Loss. It measures the difference between the predicted probabilities and the actual labels.\n",
        "\n",
        "The Log Loss cost function is defined as:\n",
        "\n",
        "L(y, p) = -[y * log(p) + (1-y) * log(1-p)]\n",
        "\n",
        "where:\n",
        "\n",
        "- L is the Log Loss\n",
        "- y is the actual label (0 or 1)\n",
        "- p is the predicted probability\n",
        "\n",
        "This cost function has several desirable properties:\n",
        "\n",
        "1. Convexity: Log Loss is a convex function, which means it has a single global minimum.\n",
        "2. Differentiability: Log Loss is differentiable, which makes it easy to optimize using gradient-based methods.\n",
        "3. Penalizes confident mistakes: Log Loss penalizes confident mistakes (i.e., predicting 0 when the actual label is 1, or vice versa) more severely than uncertain mistakes.\n",
        "\n",
        "The goal of Logistic Regression is to minimize the Log Loss cost function, which is typically done using gradient descent or other optimization algorithms."
      ],
      "metadata": {
        "id": "IZGpLbbk0rtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "ANS-> Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the Log Loss cost function. The penalty term is proportional to the magnitude of the model's coefficients.\n",
        "\n",
        "Why is regularization needed?\n",
        "\n",
        "1. Overfitting: Without regularization, the model may become too complex and fit the noise in the training data, resulting in poor performance on unseen data.\n",
        "2. Multicollinearity: When features are highly correlated, the model may assign large coefficients to some features, leading to unstable estimates.\n",
        "3. Feature selection: Regularization can help select the most relevant features by shrinking the coefficients of less important features.\n",
        "\n",
        "Types of regularization in Logistic Regression:\n",
        "\n",
        "1. L1 Regularization (Lasso): Adds a term proportional to the absolute value of the coefficients.\n",
        "2. L2 Regularization (Ridge): Adds a term proportional to the square of the coefficients.\n",
        "3. Elastic Net Regularization: Combines L1 and L2 regularization.\n",
        "\n",
        "How regularization works:\n",
        "\n",
        "1. The model learns to minimize the Log Loss cost function.\n",
        "2. The regularization term is added to the cost function, penalizing large coefficients.\n",
        "3. The model adjusts its coefficients to balance the fit to the data and the regularization penalty.\n",
        "\n",
        "Regularization helps Logistic Regression models to:\n",
        "\n",
        "1. Reduce overfitting\n",
        "2. Improve generalization\n",
        "3. Select relevant features\n",
        "4. Provide more stable estimates"
      ],
      "metadata": {
        "id": "puhrqNcI0rwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "ANS-> Lasso, Ridge, and Elastic Net are three popular regularization techniques used in linear regression to prevent overfitting. The main difference between them lies in the way they penalize the model's coefficients.\n",
        "\n",
        "Lasso Regression (L1 Regularization)\n",
        "- Penalty term: Adds a term proportional to the absolute value of the coefficients (|β|).\n",
        "- Effect: Shrinks some coefficients to zero, effectively selecting a subset of features.\n",
        "- Use case: Feature selection, sparse models, and when there are many irrelevant features.\n",
        "\n",
        "Ridge Regression (L2 Regularization)\n",
        "- Penalty term: Adds a term proportional to the square of the coefficients (β²).\n",
        "- Effect: Shrinks all coefficients by a constant factor, but doesn't set any to zero.\n",
        "- Use case: Reducing multicollinearity, stabilizing estimates, and when all features are relevant.\n",
        "\n",
        "Elastic Net Regression\n",
        "- Penalty term: Combines L1 and L2 penalties (α|β| + (1-α)β²).\n",
        "- Effect: Balances between Lasso's feature selection and Ridge's coefficient shrinkage.\n",
        "- Use case: When there are many features, some of which are correlated, and you want to select the most relevant ones."
      ],
      "metadata": {
        "id": "Pu0dQqlF0rzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "\n",
        "ANS-> Use Elastic Net instead of Lasso or Ridge in the following situations:\n",
        "\n",
        "When to prefer Elastic Net over Lasso or Ridge\n",
        "1. Correlated features\n",
        "When features are highly correlated, Lasso may randomly select one of the correlated features, while Ridge will shrink all coefficients equally. Elastic Net balances between these two extremes, selecting the most relevant features while accounting for correlations.\n",
        "\n",
        "2. Many irrelevant features\n",
        "In cases with many irrelevant features, Lasso can be too aggressive, setting many coefficients to zero. Elastic Net can provide a more nuanced approach, shrinking coefficients of irrelevant features while still selecting the most relevant ones.\n",
        "\n",
        "3. Grouped features\n",
        "When features are grouped (e.g., categorical variables with multiple levels), Elastic Net can help select the entire group or a subset of features within the group.\n",
        "\n",
        "4. Interpretability\n",
        "Elastic Net can provide more interpretable results than Lasso or Ridge, as it can identify the most relevant features while accounting for correlations and grouped features.\n",
        "\n",
        "5. Handling high-dimensional data\n",
        "In high-dimensional data (p >> n), Elastic Net can be more effective than Lasso or Ridge in selecting relevant features and handling correlations."
      ],
      "metadata": {
        "id": "2vpS2rfM0r2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "\n",
        "ANS-> The regularization parameter (λ) in Logistic Regression controls the strength of the regularization term, which adds a penalty to the Log Loss cost function for large coefficients.\n",
        "\n",
        "Impact of λ:\n",
        "\n",
        "1. Large λ: Strong regularization, leading to:\n",
        "    - Coefficients shrunk towards zero.\n",
        "    - Reduced overfitting.\n",
        "    - Simplified model.\n",
        "    - Potential underfitting.\n",
        "2. Small λ: Weak regularization, leading to:\n",
        "    - Coefficients less affected.\n",
        "    - Increased overfitting.\n",
        "    - Complex model.\n",
        "    - Potential better fit to training data.\n",
        "3. Optimal λ: Balances overfitting and underfitting, resulting in:\n",
        "    - Coefficients with optimal magnitude.\n",
        "    - Best generalization performance.\n",
        "\n",
        "Effects on model performance:\n",
        "\n",
        "1. Training accuracy: Decreases as λ increases.\n",
        "2. Validation accuracy: Initially increases, then decreases as λ increases.\n",
        "3. Test accuracy: Optimal value of λ results in best test accuracy.\n",
        "\n",
        "Choosing the optimal λ:\n",
        "\n",
        "1. Cross-validation: Use techniques like k-fold cross-validation to evaluate model performance for different values of λ.\n",
        "2. Grid search: Perform a grid search over a range of λ values to find the optimal one.\n",
        "3. Bayesian optimization: Use Bayesian optimization techniques to efficiently search for the optimal λ.\n"
      ],
      "metadata": {
        "id": "L1CVbiyE0r86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What are the key assumptions of Logistic Regression.\n",
        "\n",
        "ANS-> Logistic Regression is a widely used statistical model, but it relies on certain assumptions to ensure accurate and reliable results. Here are the key assumptions of Logistic Regression:\n",
        "\n",
        "1. Binary Dependent Variable: The dependent variable (target) should be binary, meaning it has only two possible outcomes (e.g., 0/1, yes/no, win/lose).\n",
        "2. Independence of Observations: Each observation should be independent of the others, meaning there's no correlation or clustering between observations.\n",
        "3. Linearity in the Log-Odds: The relationship between the independent variables and the log-odds of the dependent variable should be linear.\n",
        "4. No Multicollinearity: Independent variables should not be highly correlated with each other, as this can cause unstable estimates and inflated variance.\n",
        "5. Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\n",
        "6. No Significant Outliers: There should be no significant outliers in the data, as these can affect the accuracy of the model.\n",
        "7. Sufficient Sample Size: The sample size should be sufficient to provide reliable estimates of the model parameters.\n",
        "8. Correct Model Specification: The model should be correctly specified, including the correct functional form and the inclusion of relevant independent variables."
      ],
      "metadata": {
        "id": "HTae2cpT0sAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "\n",
        "ANS-> While Logistic Regression is a popular choice for classification tasks, there are many alternative algorithms that can be more suitable depending on the specific problem and dataset. Here are some alternatives:\n",
        "\n",
        "Alternatives to Logistic Regression\n",
        "1. Decision Trees: Simple, interpretable, and effective for handling categorical features.\n",
        "2. Random Forests: Ensemble method that combines multiple decision trees for improved accuracy and robustness.\n",
        "3. Support Vector Machines (SVMs): Powerful algorithm for handling high-dimensional data and non-linear relationships.\n",
        "4. K-Nearest Neighbors (KNN): Simple, intuitive algorithm for handling non-linear relationships and high-dimensional data.\n",
        "5. Gradient Boosting Machines (GBMs): Ensemble method that combines multiple weak models for improved accuracy and robustness.\n",
        "6. Neural Networks: Powerful algorithm for handling complex, non-linear relationships and high-dimensional data.\n",
        "7. Naive Bayes: Simple, probabilistic algorithm for handling categorical features and high-dimensional data.\n",
        "8. Discriminant Analysis: Linear or quadratic algorithm for handling high-dimensional data and non-linear relationships.\n",
        "\n",
        "Choosing an Alternative\n",
        "When selecting an alternative to Logistic Regression, consider the following factors:\n",
        "\n",
        "1. Dataset size and complexity: Larger datasets may require more powerful algorithms like GBMs or Neural Networks.\n",
        "2. Feature types and relationships: Categorical features may require Decision Trees or Naive Bayes, while non-linear relationships may require SVMs or Neural Networks.\n",
        "3. Interpretability and explainability: Decision Trees, Random Forests, and Naive Bayes offer more interpretable results.\n",
        "4. Computational resources: Larger datasets and more complex algorithms may require significant computational resources."
      ],
      "metadata": {
        "id": "DB0FVqjC2kmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What are Classification Evaluation Metrics.\n",
        "\n",
        "ANS-> Classification evaluation metrics are used to assess the performance of a classification model. Here are some common metrics:\n",
        "\n",
        "Accuracy Metrics\n",
        "1. Accuracy: Proportion of correctly classified instances.\n",
        "Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "2. Error Rate: Proportion of misclassified instances.\n",
        "Formula: Error Rate = (FP + FN) / (TP + TN + FP + FN)\n",
        "\n",
        "Class-Specific Metrics\n",
        "1. Precision: Proportion of true positives among all positive predictions.\n",
        "Formula: Precision = TP / (TP + FP)\n",
        "2. Recall: Proportion of true positives among all actual positive instances.\n",
        "Formula: Recall = TP / (TP + FN)\n",
        "3. F1-Score: Harmonic mean of precision and recall.\n",
        "Formula: F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "4. Specificity: Proportion of true negatives among all actual negative instances.\n",
        "Formula: Specificity = TN / (TN + FP)\n",
        "\n",
        "Other Metrics\n",
        "1. ROC-AUC (Receiver Operating Characteristic-Area Under the Curve): Measures the model's ability to distinguish between positive and negative classes.\n",
        "2. Confusion Matrix: A table that summarizes the predictions against the actual outcomes.\n",
        "3. Lift Curve: A graph that shows the model's performance at different thresholds.\n",
        "\n",
        "Threshold-Independent Metrics\n",
        "1. Area Under the Precision-Recall Curve (AUPRC): Measures the model's performance across different thresholds.\n",
        "\n",
        "When to Use Each Metric\n",
        "1. Accuracy: Use when the classes are balanced and the cost of misclassification is equal.\n",
        "2. Precision: Use when the cost of false positives is high.\n",
        "3. Recall: Use when the cost of false negatives is high.\n",
        "4. F1-Score: Use when the classes are imbalanced.\n",
        "5. ROC-AUC: Use when the classes are imbalanced or the cost of misclassification varies.\n"
      ],
      "metadata": {
        "id": "WaAk3Djb2kwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. How does class imbalance affect Logistic Regression.\n",
        "\n",
        "ANS-> Class imbalance occurs when the number of instances in one class significantly outweighs the number of instances in another class. This can affect Logistic Regression in several ways:\n",
        "\n",
        "Effects on Logistic Regression:\n",
        "\n",
        "1. Bias towards the majority class: Logistic Regression may become biased towards the majority class, resulting in poor predictive performance on the minority class.\n",
        "2. Inaccurate probability estimates: Class imbalance can lead to inaccurate probability estimates, as the model may overestimate the probability of the majority class and underestimate the probability of the minority class.\n",
        "3. Overfitting to the majority class: The model may overfit to the majority class, resulting in poor generalization performance on the minority class.\n",
        "4. Difficulty in convergence: Class imbalance can cause difficulties in convergence, as the model may oscillate between optimizing for the majority class and the minority class.\n",
        "\n",
        "Consequences:\n",
        "\n",
        "1. Poor predictive performance: Class imbalance can result in poor predictive performance on the minority class, which may be the class of interest.\n",
        "2. Inaccurate decision-making: Inaccurate probability estimates and biased predictions can lead to inaccurate decision-making.\n",
        "3. Lack of generalizability: Overfitting to the majority class can result in a lack of generalizability to new, unseen data.\n",
        "\n",
        "Techniques to address class imbalance:\n",
        "\n",
        "1. Oversampling the minority class: Creating additional instances of the minority class to balance the dataset.\n",
        "2. Undersampling the majority class: Reducing the number of instances in the majority class to balance the dataset.\n",
        "3. SMOTE (Synthetic Minority Over-sampling Technique): Creating synthetic instances of the minority class using interpolation.\n",
        "4. Class weighting: Assigning different weights to the classes during training to balance the loss function.\n",
        "5. Anomaly detection algorithms: Using algorithms specifically designed for anomaly detection, such as One-Class SVM or Local Outlier Factor (LOF)."
      ],
      "metadata": {
        "id": "QaHw1jbK2k0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "\n",
        "ANS-> Hyperparameter tuning is the process of selecting the optimal hyperparameters for a machine learning model, including Logistic Regression. Hyperparameters are parameters that are set before training the model, and they can significantly impact the model's performance.\n",
        "\n",
        "Hyperparameters in Logistic Regression:\n",
        "\n",
        "1. Regularization strength (C or λ): Controls the amount of regularization applied to the model.\n",
        "2. Penalty type (L1 or L2): Specifies the type of regularization penalty used.\n",
        "3. Max iterations: Sets the maximum number of iterations for the optimization algorithm.\n",
        "4. Tolerance: Specifies the convergence tolerance for the optimization algorithm.\n",
        "5. Class weight: Assigns different weights to different classes in the dataset.\n",
        "\n",
        "Hyperparameter tuning techniques:\n",
        "\n",
        "1. Grid search: Exhaustively searches through a predefined grid of hyperparameters.\n",
        "2. Random search: Randomly samples hyperparameters from a predefined distribution.\n",
        "3. Bayesian optimization: Uses a probabilistic approach to search for the optimal hyperparameters.\n",
        "4. Cross-validation: Evaluates the model's performance on unseen data using different hyperparameters.\n",
        "\n",
        "Why hyperparameter tuning is important:\n",
        "\n",
        "1. Improved model performance: Hyperparameter tuning can significantly improve the model's accuracy and performance.\n",
        "2. Avoiding overfitting: Hyperparameter tuning can help avoid overfitting by selecting the optimal regularization strength and other hyperparameters.\n",
        "3. Model interpretability: Hyperparameter tuning can help improve model interpretability by selecting the most relevant features and hyperparameters.\n",
        "\n",
        "Best practices for hyperparameter tuning:\n",
        "\n",
        "1. Use a robust evaluation metric: Use a metric that accurately reflects the model's performance, such as accuracy or F1-score.\n",
        "2. Use a suitable search space: Define a search space that includes a range of plausible hyperparameters.\n",
        "3. Use a efficient search algorithm: Choose a search algorithm that efficiently explores the search space, such as Bayesian optimization or random search.\n",
        "4. Monitor and adjust: Monitor the tuning process and adjust the search space or algorithm as needed."
      ],
      "metadata": {
        "id": "IRBTzT7s2k3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "\n",
        "ANS-> In Logistic Regression, a solver is an optimization algorithm used to find the optimal parameters that minimize the loss function. Here are some common solvers used in Logistic Regression:\n",
        "\n",
        "Solvers in Logistic Regression\n",
        "1. Liblinear: A linear solver that uses the LIBLINEAR library. It's suitable for small to medium-sized datasets.\n",
        "2. SAG (Stochastic Average Gradient): A stochastic solver that uses the average gradient to update the parameters. It's suitable for large datasets.\n",
        "3. SAGA (Stochastic Average Gradient Algorithm): A variant of SAG that uses a different update rule. It's also suitable for large datasets.\n",
        "4. LBFGS (Limited-memory BFGS): A quasi-Newton solver that uses the BFGS algorithm with limited memory. It's suitable for medium to large-sized datasets.\n",
        "5. Newton: A Newton solver that uses the exact Hessian matrix to update the parameters. It's suitable for small to medium-sized datasets.\n",
        "\n",
        "Choosing a Solver\n",
        "When choosing a solver, consider the following factors:\n",
        "\n",
        "1. Dataset size: For small datasets, Liblinear or Newton may be suitable. For large datasets, SAG, SAGA, or LBFGS may be more efficient.\n",
        "2. Computational resources: If computational resources are limited, a stochastic solver like SAG or SAGA may be more suitable.\n",
        "3. Convergence speed: If convergence speed is critical, LBFGS or Newton may be more suitable.\n",
        "4. Regularization: If regularization is used, a solver that can handle regularization, such as Liblinear or LBFGS, may be more suitable.\n",
        "\n",
        "Default Solver\n",
        "The default solver in scikit-learn, a popular Python machine learning library, is Liblinear for small datasets and SAG or SAGA for large datasets.\n",
        "\n",
        "Best Practices\n",
        "1. Start with the default solver: Try the default solver first, and then experiment with other solvers if necessary.\n",
        "2. Monitor convergence: Monitor the convergence of the solver, and adjust the solver or hyperparameters as needed.\n",
        "3. Experiment with different solvers: Try different solvers to see which one works best for your specific problem."
      ],
      "metadata": {
        "id": "K14dodGL2k6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. How is Logistic Regression extended for multiclass classification.\n",
        "\n",
        "ANS-> Logistic Regression can be extended for multiclass classification using several techniques:\n",
        "\n",
        "Techniques for Multiclass Classification\n",
        "1. One-vs-All (OvA) Approach\n",
        "In this approach, a separate Logistic Regression model is trained for each class, with the class label as the positive class and all other classes as the negative class. The class with the highest predicted probability is selected as the final prediction.\n",
        "\n",
        "2. One-vs-One (OvO) Approach\n",
        "In this approach, a separate Logistic Regression model is trained for each pair of classes. The class with the highest number of wins is selected as the final prediction.\n",
        "\n",
        "3. Multinomial Logistic Regression\n",
        "This approach extends the traditional Logistic Regression model to handle multiple classes. The model predicts the probability of each class, and the class with the highest probability is selected as the final prediction.\n",
        "\n",
        "4. Softmax Regression\n",
        "Softmax Regression is a special case of Multinomial Logistic Regression. It uses the softmax function to normalize the output probabilities, ensuring that they sum to 1.\n",
        "\n",
        "Key Considerations\n",
        "When extending Logistic Regression for multiclass classification, consider the following:\n",
        "\n",
        "- Number of classes: The choice of technique depends on the number of classes. OvA and OvO are suitable for a small number of classes, while Multinomial Logistic Regression and Softmax Regression are more suitable for a large number of classes.\n",
        "- Class balance: If the classes are imbalanced, techniques like OvA and OvO may not perform well. Multinomial Logistic Regression and Softmax Regression can handle class imbalance more effectively.\n",
        "- Computational complexity: The computational complexity of the techniques varies. OvA and OvO require training multiple models, while Multinomial Logistic Regression and Softmax Regression require training a single model.\n"
      ],
      "metadata": {
        "id": "yVcI2IzY2k9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. What are the advantages and disadvantages of Logistic Regression.\n",
        "\n",
        "ANS-> Logistic Regression is a popular machine learning algorithm for binary classification tasks. Here are its advantages and disadvantages:\n",
        "\n",
        "Advantages\n",
        "1. Interpretability: Logistic Regression provides interpretable results, making it easier to understand the relationship between the features and the target variable.\n",
        "2. Easy to implement: Logistic Regression is a simple algorithm to implement, and its implementation is widely available in most machine learning libraries.\n",
        "3. Fast computation: Logistic Regression is computationally efficient and can be trained quickly, even on large datasets.\n",
        "4. Handling categorical variables: Logistic Regression can handle categorical variables directly, without the need for preprocessing.\n",
        "5. Providing probabilities: Logistic Regression provides probabilities for each class, making it easier to evaluate the uncertainty of the predictions.\n",
        "\n",
        "Disadvantages\n",
        "1. Assumes linear relationship: Logistic Regression assumes a linear relationship between the features and the log-odds of the target variable, which may not always be the case.\n",
        "2. Not suitable for complex relationships: Logistic Regression may not be effective in modeling complex relationships between the features and the target variable.\n",
        "3. Sensitive to outliers: Logistic Regression can be sensitive to outliers in the data, which can affect the accuracy of the model.\n",
        "4. Not suitable for multi-class problems: Logistic Regression is typically used for binary classification problems and may not be effective for multi-class problems.\n",
        "5. Can suffer from overfitting: Logistic Regression can suffer from overfitting, especially when the number of features is large compared to the number of samples."
      ],
      "metadata": {
        "id": "jlUGLCwA2lA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. What are some use cases of Logistic Regression.\n",
        "\n",
        "ANS-> Logistic Regression is a versatile algorithm that can be applied to various domains and problems. Here are some use cases of Logistic Regression:\n",
        "\n",
        "Use Cases\n",
        "1. Credit Risk Assessment: Predicting the likelihood of loan defaults or credit card defaults based on customer data.\n",
        "2. Medical Diagnosis: Predicting the likelihood of diseases or conditions based on patient data, such as age, sex, and medical history.\n",
        "3. Customer Churn Prediction: Predicting the likelihood of customers switching to a competitor based on their behavior and demographics.\n",
        "4. Spam Detection: Classifying emails as spam or not spam based on their content and metadata.\n",
        "5. Image Classification: Classifying images into different categories, such as objects, scenes, or actions.\n",
        "6. Sentiment Analysis: Predicting the sentiment of text data, such as positive, negative, or neutral.\n",
        "7. Recommendation Systems: Predicting the likelihood of a user liking or disliking a product based on their past behavior.\n",
        "8. Fraud Detection: Predicting the likelihood of fraudulent transactions based on transaction data.\n",
        "9. Marketing Response Prediction: Predicting the likelihood of customers responding to marketing campaigns based on their demographics and behavior.\n",
        "10. Social Media Analysis: Predicting the likelihood of social media posts being engaging or not based on their content and metadata.\n"
      ],
      "metadata": {
        "id": "mt6_8NqC2lEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "\n",
        "ANS-> Softmax Regression and Logistic Regression are both supervised learning algorithms used for classification tasks. However, there are key differences between them:\n",
        "\n",
        "Logistic Regression:\n",
        "\n",
        "1. Binary classification: Logistic Regression is used for binary classification problems, where the target variable has only two classes (e.g., 0/1, yes/no).\n",
        "2. Sigmoid function: Logistic Regression uses the sigmoid function to model the probability of the positive class.\n",
        "3. Output: The output of Logistic Regression is a probability value between 0 and 1, indicating the likelihood of the positive class.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "1. Multi-class classification: Softmax Regression is used for multi-class classification problems, where the target variable has more than two classes (e.g., handwritten digit recognition).\n",
        "2. Softmax function: Softmax Regression uses the softmax function to model the probability distribution over all classes.\n",
        "3. Output: The output of Softmax Regression is a probability distribution over all classes, where each class has a probability value between 0 and 1."
      ],
      "metadata": {
        "id": "tu2QgbGl2lIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "\n",
        "ANS-> Choosing between One-vs-Rest (OvR) and Softmax for multiclass classification depends on several factors, including the nature of the problem, the number of classes, and the desired output. Here are some guidelines to help you decide:\n",
        "\n",
        "One-vs-Rest (OvR)\n",
        "Advantages\n",
        "1. Easy to implement: OvR is a straightforward extension of binary classification.\n",
        "2. Flexible: Can be used with any binary classification algorithm.\n",
        "\n",
        "Disadvantages\n",
        "1. Class imbalance: OvR can be sensitive to class imbalance, as the binary classifier may be biased towards the majority class.\n",
        "2. Inconsistent predictions: OvR can produce inconsistent predictions, as the binary classifier may predict different classes for the same input.\n",
        "\n",
        "Softmax\n",
        "Advantages\n",
        "1. Multiclass-aware: Softmax is designed for multiclass classification and can handle class imbalance more effectively.\n",
        "2. Consistent predictions: Softmax produces consistent predictions, as the output is a probability distribution over all classes.\n",
        "\n",
        "Disadvantages\n",
        "1. Computationally expensive: Softmax can be computationally expensive, especially for large datasets.\n",
        "2. Assumes mutual exclusivity: Softmax assumes that the classes are mutually exclusive, which may not always be the case.\n",
        "\n",
        "Choosing between OvR and Softmax\n",
        "1. Number of classes: If there are only a few classes (e.g., 3-5), OvR might be sufficient. For larger numbers of classes, Softmax is generally preferred.\n",
        "2. Class imbalance: If the classes are imbalanced, Softmax is more robust and can handle class imbalance more effectively.\n",
        "3. Desired output: If you need a probability distribution over all classes, Softmax is the better choice. If you only need a binary decision, OvR might be sufficient.\n",
        "4. Computational resources: If computational resources are limited, OvR might be faster and more efficient."
      ],
      "metadata": {
        "id": "yP3Unnsv2lNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "ANS-> Interpreting coefficients in Logistic Regression can be a bit tricky, but I'll break it down for you.\n",
        "\n",
        "What do coefficients represent in Logistic Regression?\n",
        "\n",
        "In Logistic Regression, coefficients represent the change in the log-odds of the outcome variable for a one-unit change in the predictor variable, while holding all other predictor variables constant.\n",
        "\n",
        "How to interpret coefficients:\n",
        "\n",
        "1. Positive coefficients: A positive coefficient indicates that an increase in the predictor variable is associated with an increase in the log-odds of the outcome variable. In other words, the predictor variable is positively related to the outcome variable.\n",
        "2. Negative coefficients: A negative coefficient indicates that an increase in the predictor variable is associated with a decrease in the log-odds of the outcome variable. In other words, the predictor variable is negatively related to the outcome variable.\n",
        "3. Magnitude of coefficients: The magnitude of the coefficient represents the strength of the relationship between the predictor variable and the outcome variable. A larger coefficient indicates a stronger relationship.\n",
        "\n",
        "Interpreting coefficients in terms of odds ratios:\n",
        "\n",
        "Odds ratios are often used to interpret the results of Logistic Regression. An odds ratio represents the change in the odds of the outcome variable for a one-unit change in the predictor variable.\n",
        "\n",
        "To calculate the odds ratio, you can exponentiate the coefficient:\n",
        "\n",
        "odds ratio = exp(coefficient)\n",
        "\n",
        "For example, if the coefficient is 0.5, the odds ratio would be:\n",
        "\n",
        "odds ratio = exp(0.5) ≈ 1.65\n",
        "\n",
        "This means that a one-unit increase in the predictor variable is associated with a 65% increase in the odds of the outcome variable.\n",
        "\n",
        "Common mistakes to avoid:\n",
        "\n",
        "1. Interpreting coefficients as probabilities: Coefficients in Logistic Regression represent changes in log-odds, not probabilities.\n",
        "2. Ignoring the non-linear relationship: Logistic Regression assumes a non-linear relationship between the predictor variables and the outcome variable. Ignoring this non-linearity can lead to incorrect interpretations."
      ],
      "metadata": {
        "id": "5TleKW4I4hB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "kbj_lAzr4hTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy"
      ],
      "metadata": {
        "id": "eB4NFORp4hel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjDXx8xE4zZM",
        "outputId": "656ee7c0-f563-4f38-bb6b-1b41d9d7765d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy"
      ],
      "metadata": {
        "id": "m13RbD7I5BFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the range of regularization strengths (C values)\n",
        "C_values = np.logspace(-4, 4, 20)\n",
        "\n",
        "# Initialize a list to store the model accuracies\n",
        "accuracies = []\n",
        "\n",
        "# Iterate over the range of regularization strengths\n",
        "for C in C_values:\n",
        "    # Create a Logistic Regression object with L1 regularization (Lasso)\n",
        "    logreg_lasso = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, C=C)\n",
        "\n",
        "    # Train the model using the training data\n",
        "    logreg_lasso.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the testing data\n",
        "    y_pred_lasso = logreg_lasso.predict(X_test)\n",
        "\n",
        "    # Calculate the model accuracy\n",
        "    accuracy_lasso = accuracy_score(y_test, y_pred_lasso)\n",
        "\n",
        "    # Append the model accuracy to the list\n",
        "    accuracies.append(accuracy_lasso)\n",
        "\n",
        "# Print the model accuracy for each regularization strength\n",
        "for C, accuracy in zip(C_values, accuracies):\n",
        "    print(f\"C={C:.4f}, Accuracy={accuracy:.2f}\")\n",
        "\n",
        "# Find the regularization strength with the highest model accuracy\n",
        "best_C = C_values[np.argmax(accuracies)]\n",
        "best_accuracy = np.max(accuracies)\n",
        "\n",
        "print(f\"\\nBest C value: {best_C:.4f}\")\n",
        "print(f\"Best model accuracy: {best_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djRKcoE85BSF",
        "outputId": "7a4a4799-8627-44a7-ec11-d4e5785bbdca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.0001, Accuracy=0.33\n",
            "C=0.0003, Accuracy=0.33\n",
            "C=0.0007, Accuracy=0.33\n",
            "C=0.0018, Accuracy=0.33\n",
            "C=0.0048, Accuracy=0.33\n",
            "C=0.0127, Accuracy=0.37\n",
            "C=0.0336, Accuracy=0.70\n",
            "C=0.0886, Accuracy=0.87\n",
            "C=0.2336, Accuracy=0.87\n",
            "C=0.6158, Accuracy=1.00\n",
            "C=1.6238, Accuracy=1.00\n",
            "C=4.2813, Accuracy=1.00\n",
            "C=11.2884, Accuracy=1.00\n",
            "C=29.7635, Accuracy=1.00\n",
            "C=78.4760, Accuracy=1.00\n",
            "C=206.9138, Accuracy=1.00\n",
            "C=545.5595, Accuracy=1.00\n",
            "C=1438.4499, Accuracy=1.00\n",
            "C=3792.6902, Accuracy=1.00\n",
            "C=10000.0000, Accuracy=1.00\n",
            "\n",
            "Best C value: 0.6158\n",
            "Best model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "GY5GL4-r5Bd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the range of regularization strengths (C values) and Elastic Net mixing parameter (l1_ratio)\n",
        "C_values = [0.1, 1, 10]\n",
        "l1_ratios = [0.2, 0.5, 0.8]\n",
        "\n",
        "# Iterate over the range of regularization strengths and Elastic Net mixing parameter\n",
        "for C in C_values:\n",
        "    for l1_ratio in l1_ratios:\n",
        "        # Create a Logistic Regression object with Elastic Net Regularization\n",
        "        logreg_elasticnet = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000, C=C, l1_ratio=l1_ratio)\n",
        "\n",
        "        # Train the model using the training data\n",
        "        logreg_elasticnet.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on the testing data\n",
        "        y_pred_elasticnet = logreg_elasticnet.predict(X_test)\n",
        "\n",
        "        # Calculate the model accuracy\n",
        "        accuracy_elasticnet = accuracy_score(y_test, y_pred_elasticnet)\n",
        "\n",
        "        # Print the model accuracy and coefficients\n",
        "        print(f\"C={C:.2f}, l1_ratio={l1_ratio:.2f}\")\n",
        "        print(f\"Model Accuracy: {accuracy_elasticnet:.2f}\")\n",
        "        print(\"Model Coefficients:\")\n",
        "        print(logreg_elasticnet.coef_)\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHeY97DM5Bqz",
        "outputId": "a47c6e60-ee45-4fb3-e5ec-2489c12d90c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.10, l1_ratio=0.20\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[-0.09958041  0.22371508 -1.19124871 -0.33179135]\n",
            " [ 0.         -0.20367772  0.          0.        ]\n",
            " [ 0.02146522  0.          1.03878168  0.54521501]]\n",
            "\n",
            "C=0.10, l1_ratio=0.50\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.          0.         -1.42400313  0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          1.1336803   0.34570875]]\n",
            "\n",
            "C=0.10, l1_ratio=0.80\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.          0.         -1.50147117  0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          1.30557164  0.        ]]\n",
            "\n",
            "C=1.00, l1_ratio=0.20\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.16522662  1.39384643 -2.30032112 -0.89813135]\n",
            " [ 0.17213178 -0.17368168 -0.01658117 -0.71401919]\n",
            " [-0.58735749 -0.97016566  2.56635887  1.8621496 ]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=1.00, l1_ratio=0.50\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.02834038  1.59696464 -2.42639328 -0.59307696]\n",
            " [ 0.          0.          0.         -0.5045682 ]\n",
            " [-0.77998387 -0.95688689  2.74518055  2.09727018]]\n",
            "\n",
            "C=1.00, l1_ratio=0.80\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.          1.8035659  -2.57791166  0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.95830244 -1.04382695  3.04789616  2.65369823]]\n",
            "\n",
            "C=10.00, l1_ratio=0.20\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[ 1.2224184   2.9600623  -4.011481   -1.83427212]\n",
            " [ 0.46649882  0.29986248 -0.12858659 -1.89640296]\n",
            " [-1.82517523 -3.37540389  4.27214954  3.86674833]]\n",
            "\n",
            "C=10.00, l1_ratio=0.50\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[ 1.16699615  3.08332408 -4.16278141 -1.77360884]\n",
            " [ 0.351271    0.22615397  0.         -1.99702269]\n",
            " [-1.96883112 -3.70217927  4.56322458  4.22098242]]\n",
            "\n",
            "C=10.00, l1_ratio=0.80\n",
            "Model Accuracy: 1.00\n",
            "Model Coefficients:\n",
            "[[ 1.00980698  3.23404584 -4.52921433 -1.64317531]\n",
            " [ 0.14611797  0.02406658  0.         -2.06207287]\n",
            " [-2.25332698 -4.25293126  4.86962511  4.80056845]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')"
      ],
      "metadata": {
        "id": "jCgxXQpQ5B4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Elastic Net Regularization parameters\n",
        "C = 1.0\n",
        "l1_ratio = 0.5\n",
        "\n",
        "# Create a Logistic Regression object with Elastic Net Regularization\n",
        "logreg_elasticnet = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000, C=C, l1_ratio=l1_ratio)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg_elasticnet.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_elasticnet = logreg_elasticnet.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred_elasticnet)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_elasticnet))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_elasticnet))\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(logreg_elasticnet.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r79PWo-d5CIr",
        "outputId": "ee1b9365-e0ce-415d-fc63-3f288c0fc359"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Model Coefficients:\n",
            "[[ 0.02667602  1.59600513 -2.42635306 -0.59295857]\n",
            " [ 0.          0.          0.         -0.50465626]\n",
            " [-0.78002861 -0.95694947  2.7451745   2.09724262]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'C"
      ],
      "metadata": {
        "id": "wFlZN5aI5CUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object for multiclass classification using one-vs-rest (OvR)\n",
        "logreg_ovr = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_ovr = logreg_ovr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(\"OvR Model Accuracy:\", accuracy_ovr)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_ovr))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_ovr))\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"OvR Model Coefficients:\")\n",
        "print(logreg_ovr.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8aqq8Kt5Ce0",
        "outputId": "3eb49131-9de4-4179-e0dd-8a2019034266"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR Model Accuracy: 0.9666666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      0.89      0.94         9\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "OvR Model Coefficients:\n",
            "[[-0.42762216  0.88771927 -2.21471658 -0.91610036]\n",
            " [-0.03387836 -2.0442989   0.54266011 -1.0179372 ]\n",
            " [-0.38904645 -0.62147609  2.7762982   2.09067085]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracyC"
      ],
      "metadata": {
        "id": "2QxBJpNS5Cp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter tuning space\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5)\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the testing set\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Testing Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JARDwe3b5Cz0",
        "outputId": "5da5894d-cc4c-4d61-e348-a50de6082852"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best Accuracy: 0.9666666666666666\n",
            "Testing Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "15 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.93333333        nan 0.96666667        nan 0.94166667]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy"
      ],
      "metadata": {
        "id": "dUutuFwX5C-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create a Stratified K-Fold Cross-Validation object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Initialize a list to store the accuracy scores\n",
        "accuracies = []\n",
        "\n",
        "# Iterate over the folds\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model using the training data\n",
        "    logreg.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the testing data\n",
        "    y_pred = logreg.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Append the accuracy score to the list\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate the average accuracy\n",
        "average_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(\"Average Accuracy:\", average_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AygPeR9Y5DH0",
        "outputId": "9dc4aa3e-aa01-4835-83ed-fd1c4183564c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "cKg53US15DZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the pandas library if it is not installed\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Replace 'your_file.csv' with the actual file path\n",
        "file_path = '/content/sample_data/california_housing_test.csv'  # Replace with your actual file path\n",
        "try:\n",
        "    data = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '/content/sample_data/california_housing_test.csv' not found. Please provide the correct file path.\")\n",
        "    exit() # This exit() statement should be at the same indentation level as the except block\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    exit() # This exit() statement should be at the same indentation level as the except block\n",
        "\n",
        "# Assuming the last column is the target variable\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Convert non-numeric columns to numerical representations using one-hot encoding\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cYB4Ni45Dkv",
        "outputId": "c45d01f3-16fa-457d-af88-d70541774840"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Accuracy: 0.023333333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracyM"
      ],
      "metadata": {
        "id": "MJVQ2Wub5D2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter tuning space\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(logreg, param_grid, cv=5, n_iter=10, random_state=42)\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the testing set\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Testing Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7_D_cjo5EAE",
        "outputId": "b6c0b3cc-fcbe-44f9-84fa-e6aa4bfbd8c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 10}\n",
            "Best Accuracy: 0.9666666666666668\n",
            "Testing Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "vxSRQi0GE27W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a LabelEncoder object to convert the target variable into a numerical format\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "# Define the number of classes\n",
        "n_classes = len(set(y_train))\n",
        "\n",
        "# Initialize a list to store the OvO models\n",
        "ovo_models = []\n",
        "\n",
        "# Train OvO models\n",
        "for i in range(n_classes):\n",
        "    for j in range(i + 1, n_classes):\n",
        "        # Create a binary target variable\n",
        "        y_train_binary = (y_train == i).astype(int) | ((y_train == j) * 2).astype(int)\n",
        "\n",
        "        # Create a Logistic Regression object\n",
        "        logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "        # Train the model using the training data\n",
        "        logreg.fit(X_train, y_train_binary)\n",
        "\n",
        "        # Append the model to the list\n",
        "        ovo_models.append((i, j, logreg))\n",
        "\n",
        "# Make predictions using the OvO models\n",
        "y_pred = []\n",
        "for x in X_test:\n",
        "    votes = [0] * n_classes\n",
        "    for i, j, model in ovo_models:\n",
        "        pred = model.predict([x])[0]\n",
        "        if pred == 0:\n",
        "            votes[i] += 1\n",
        "        elif pred == 2:\n",
        "            votes[j] += 1\n",
        "    y_pred.append(votes.index(max(votes)))\n",
        "\n",
        "# Evaluate the OvO model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"OvO Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpdyK6NOE3GQ",
        "outputId": "905d7477-e25b-460f-a0b0-3fba7f488d7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvO Model Accuracy: 0.36666666666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "Mxi6YUQqE330"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "CqTJkKzgE4nn",
        "outputId": "a4ddb008-7e6c-42c6-b1aa-5b7a82c282d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score"
      ],
      "metadata": {
        "id": "d1cJQmP4E46R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Oip-S-MPE5EV",
        "outputId": "245cf8cd-9a12-48c4-a4a1-bb6a37fdea7e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance:\n",
            "Accuracy: 0.96\n",
            "Precision: 0.95\n",
            "Recall: 0.99\n",
            "F1-Score: 0.97\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94        43\n",
            "           1       0.95      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[39  4]\n",
            " [ 1 70]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "qtBYB3WsE5QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "# Create a Logistic Regression object with class weights\n",
        "logreg = LogisticRegression(class_weight={0: class_weights[0], 1: class_weights[1]}, max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbM4nq6XE5Zq",
        "outputId": "43ce6a4f-3d8b-4f21-ec9b-43c739a6549b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance:\n",
            "Accuracy: 0.95\n",
            "Precision: 0.69\n",
            "Recall: 1.00\n",
            "F1-Score: 0.82\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97       180\n",
            "           1       0.69      1.00      0.82        20\n",
            "\n",
            "    accuracy                           0.95       200\n",
            "   macro avg       0.84      0.97      0.90       200\n",
            "weighted avg       0.97      0.95      0.96       200\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[171   9]\n",
            " [  0  20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance"
      ],
      "metadata": {
        "id": "tPZ4oJv0E5l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "titanic['Age'] = titanic['Age'].fillna(titanic['Age'].mean())\n",
        "titanic['Fare'] = titanic['Fare'].fillna(titanic['Fare'].mean())\n",
        "titanic['Embarked'] = titanic['Embarked'].fillna(titanic['Embarked'].mode()[0])\n",
        "\n",
        "# Define features and target\n",
        "X = titanic.drop(['Survived', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "y = titanic['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing steps\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                         ('logreg', LogisticRegression(max_iter=1000))])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcJQTIfVE5wV",
        "outputId": "18e6ac2a-0de4-4496-c65d-13a8b35fd1c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance:\n",
            "Accuracy: 0.80\n",
            "Precision: 0.77\n",
            "Recall: 0.74\n",
            "F1-Score: 0.76\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.84       105\n",
            "           1       0.77      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.80      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[89 16]\n",
            " [19 55]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling"
      ],
      "metadata": {
        "id": "uOhf5fEkHGDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object without scaling\n",
        "logreg_without_scaling = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model without scaling\n",
        "logreg_without_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions without scaling\n",
        "y_pred_without_scaling = logreg_without_scaling.predict(X_test)\n",
        "\n",
        "# Evaluate the model without scaling\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred_without_scaling)\n",
        "print(\"Accuracy without scaling:\", accuracy_without_scaling)\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression object with scaling\n",
        "logreg_with_scaling = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model with scaling\n",
        "logreg_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions with scaling\n",
        "y_pred_with_scaling = logreg_with_scaling.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model with scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_with_scaling)\n",
        "\n",
        "# Compare the results\n",
        "print(\"Comparison of results:\")\n",
        "print(\"Accuracy without scaling:\", accuracy_without_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_with_scaling)\n",
        "\n",
        "# Print the classification report and confusion matrix for both models\n",
        "print(\"\\nClassification Report without scaling:\")\n",
        "print(classification_report(y_test, y_pred_without_scaling))\n",
        "print(\"Confusion Matrix without scaling:\")\n",
        "print(confusion_matrix(y_test, y_pred_without_scaling))\n",
        "\n",
        "print(\"\\nClassification Report with scaling:\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))\n",
        "print(\"Confusion Matrix with scaling:\")\n",
        "print(confusion_matrix(y_test, y_pred_with_scaling))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XfdatLwHGNr",
        "outputId": "a6dd5e40-4d28-41c0-a0e9-8599b090af88"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.0\n",
            "Accuracy with scaling: 1.0\n",
            "Comparison of results:\n",
            "Accuracy without scaling: 1.0\n",
            "Accuracy with scaling: 1.0\n",
            "\n",
            "Classification Report without scaling:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix without scaling:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Classification Report with scaling:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix with scaling:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "gPhR0X2qHGbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Evaluate the model's performance using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"Model ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label='ROC Curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RqeDRO0OHGlJ",
        "outputId": "72ca4962-78f9-44e4-ff9f-3c727dcc984d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n",
            "Model ROC-AUC Score: 0.9977071732721913\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94        43\n",
            "           1       0.95      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[39  4]\n",
            " [ 1 70]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "zoFM9cutHGwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object with a custom learning rate (C=0.5)\n",
        "logreg = LogisticRegression(C=0.5, max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5PI-SmOHG7o",
        "outputId": "2d0ad00e-5da8-42a9-eba3-27e59fdf5198"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients"
      ],
      "metadata": {
        "id": "6VoMW1muHHFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create a DataFrame with feature names\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Get the model coefficients\n",
        "coefficients = logreg.coef_\n",
        "\n",
        "# Create a DataFrame with feature names and coefficients\n",
        "# The original code had a shape mismatch. coefficients.T is used to transpose\n",
        "# the coefficients array so that it has 3 rows (one for each class)\n",
        "# and 4 columns (one for each feature). This matches the shape expected\n",
        "# by pd.DataFrame, where the index is the feature names and the columns\n",
        "# represent the coefficients for each class.\n",
        "df_coefficients = pd.DataFrame(coefficients.T, columns=['Coeff_Class_0', 'Coeff_Class_1', 'Coeff_Class_2'], index=iris.feature_names)\n",
        "\n",
        "\n",
        "# Sort the DataFrame by coefficient magnitude for class 0\n",
        "df_coefficients = df_coefficients.sort_values(by=\"Coeff_Class_0\", key=abs, ascending=False)\n",
        "\n",
        "# Print the top 3 features with the highest coefficient magnitude for class 0\n",
        "print(\"\\nTop 3 Features by Coefficient Magnitude for Class 0:\")\n",
        "print(df_coefficients.head(3))\n",
        "\n",
        "# Print the bottom 3 features with the lowest coefficient magnitude for class 0\n",
        "print(\"\\nBottom 3 Features by Coefficient Magnitude for Class 0:\")\n",
        "print(df_coefficients.tail(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDtAsiQHHHOz",
        "outputId": "d087a198-d34b-4446-9424-1dac84686bed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Top 3 Features by Coefficient Magnitude for Class 0:\n",
            "                   Coeff_Class_0  Coeff_Class_1  Coeff_Class_2\n",
            "petal length (cm)      -2.375108      -0.213014       2.588121\n",
            "petal width (cm)       -0.998746      -0.775755       1.774501\n",
            "sepal width (cm)        0.962586      -0.254865      -0.707721\n",
            "\n",
            "Bottom 3 Features by Coefficient Magnitude for Class 0:\n",
            "                   Coeff_Class_0  Coeff_Class_1  Coeff_Class_2\n",
            "petal width (cm)       -0.998746      -0.775755       1.774501\n",
            "sepal width (cm)        0.962586      -0.254865      -0.707721\n",
            "sepal length (cm)      -0.393402       0.508404      -0.115002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "Olp6P54kHHZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, cohen_kappa_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Evaluate the model's performance using Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\nCohen's Kappa Score:\", kappa)\n",
        "\n",
        "# Interpret the Cohen's Kappa Score\n",
        "kappa_interpretation = np.select([kappa < 0, kappa < 0.2, kappa < 0.4, kappa < 0.6, kappa < 0.8, kappa < 1],\n",
        "                                    [\"No agreement\", \"Slight agreement\", \"Fair agreement\", \"Moderate agreement\", \"Substantial agreement\", \"Almost perfect agreement\"],\n",
        "                                    default=\"Perfect agreement\")\n",
        "print(\"\\nKappa Interpretation:\", kappa_interpretation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5H8F-R8HHix",
        "outputId": "12f8fe9c-9032-4ef5-e398-3007f82df8f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Cohen's Kappa Score: 1.0\n",
            "\n",
            "Kappa Interpretation: Perfect agreement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "sjgMYqp0HHq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, auc, roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate a binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model's performance using precision and recall\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(\"Model Precision:\", precision)\n",
        "print(\"Model Recall:\", recall)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "precision_curve, recall_curve, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall_curve, precision_curve, marker='.', label='Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate the area under the Precision-Recall Curve (AUPRC)\n",
        "auprc = auc(recall_curve, precision_curve)\n",
        "print(\"Area under the Precision-Recall Curve (AUPRC):\", auprc)\n",
        "\n",
        "# Plot the ROC Curve for comparison\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, marker='.', label='ROC Curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate the area under the ROC Curve (AUC-ROC)\n",
        "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"Area under the ROC Curve (AUC-ROC):\", auc_roc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9wHtW-lqHH0S",
        "outputId": "49061145-87ff-4cc6-8dc4-f832b2eb737f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Precision: 0.9484536082474226\n",
            "Model Recall: 0.9292929292929293\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area under the Precision-Recall Curve (AUPRC): 0.9847932390734637\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIjCAYAAAAQgZNYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAS6pJREFUeJzt3Xt8z/X///H7e5v3DtjQ2IFpDhGRMx+E1Go6iCgrPlk+0lHU+ORQrKmscuygdJIOypDKJ8UXpSKdsJBTTgmb7CMbMxt7P39/9PP+9G4H7/dse+81t+vl8r7k9Xw/n6/X4+V10e5enq/ny2aMMQIAAAAsyMfbBQAAAAAlRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAaAQc+fOlc1mc378/PxUt25d3XnnnTp48GChY4wxeuedd9S9e3fVqFFDQUFBatmypSZNmqTs7Owij/Xhhx/quuuuU2hoqOx2uyIjIzVgwAB9/vnnbtV66tQpzZgxQ506dVJISIgCAgLUpEkTDR8+XDt37izR+QOAVdiMMcbbRQBARTN37lwNGTJEkyZNUoMGDXTq1Cl9++23mjt3rqKjo7VlyxYFBAQ4++fn52vgwIFasGCBunXrpn79+ikoKEhff/213nvvPTVv3lwrV65UWFiYc4wxRv/61780d+5ctWnTRrfccovCw8OVlpamDz/8UOvXr9fatWvVpUuXIuvMyMhQr169tH79et14442KiYlRtWrVtGPHDs2fP1/p6enKy8sr098rAPAqAwAo4M033zSSzA8//ODSPmbMGCPJpKSkuLRPnjzZSDKjR48usK8lS5YYHx8f06tXL5f2KVOmGEnmoYceMg6Ho8C4t99+23z33XfF1nnDDTcYHx8fs2jRogLfnTp1yowaNarY8e46ffq0yc3NLZV9AUBpYpoBAHigW7dukqTdu3c723JycjRlyhQ1adJEycnJBcb07t1b8fHxWrZsmb799lvnmOTkZF166aWaOnWqbDZbgXF33HGHOnbsWGQt3333nZYuXaqhQ4eqf//+Bb739/fX1KlTndtXXnmlrrzyygL97rzzTkVHRzu39+3bJ5vNpqlTp2rmzJlq1KiR/P39tXHjRvn5+SkpKanAPnbs2CGbzaYXX3zR2Xbs2DE99NBDioqKkr+/vxo3bqxnnnlGDoejyHMCAE8RZgHAA/v27ZMk1axZ09m2Zs0a/fHHHxo4cKD8/PwKHTd48GBJ0ieffOIcc/ToUQ0cOFC+vr4lqmXJkiWS/gy9ZeHNN9/UCy+8oLvvvlvTpk1TRESEevTooQULFhTom5KSIl9fX916662SpJMnT6pHjx569913NXjwYD3//PPq2rWrxo0bp4SEhDKpF8CFqfD/6wIAJEmZmZnKyMjQqVOn9N133ykpKUn+/v668cYbnX22bt0qSWrVqlWR+zn73bZt21z+27JlyxLXVhr7KM6BAwe0a9cu1a5d29kWFxene+65R1u2bFGLFi2c7SkpKerRo4dzTvD06dO1e/dubdy4UZdccokk6Z577lFkZKSmTJmiUaNGKSoqqkzqBnBh4c4sABQjJiZGtWvXVlRUlG655RZVrVpVS5YsUb169Zx9jh8/LkmqXr16kfs5+11WVpbLf4sbcy6lsY/i9O/f3yXISlK/fv3k5+enlJQUZ9uWLVu0detWxcXFOdsWLlyobt26qWbNmsrIyHB+YmJilJ+fr6+++qpMagZw4eHOLAAUY9asWWrSpIkyMzM1Z84cffXVV/L393fpczZMng21hfl74A0ODj7nmHP56z5q1KhR4v0UpUGDBgXaQkNDdfXVV2vBggV64oknJP15V9bPz0/9+vVz9vvll1+0adOmAmH4rN9//73U6wVwYSLMAkAxOnbsqPbt20uS+vbtqyuuuEIDBw7Ujh07VK1aNUlSs2bNJEmbNm1S3759C93Ppk2bJEnNmzeXJF166aWSpM2bNxc55lz+uo+zD6YVx2azyRSyGmN+fn6h/QMDAwttv+222zRkyBClpqaqdevWWrBgga6++mqFhoY6+zgcDl1zzTV65JFHCt1HkyZNzlkvALiDaQYA4CZfX18lJyfr0KFDLk/tX3HFFapRo4bee++9IoPh22+/LUnOubZXXHGFatasqffff7/IMefSu3dvSdK7777rVv+aNWvq2LFjBdp//fVXj47bt29f2e12paSkKDU1VTt37tRtt93m0qdRo0Y6ceKEYmJiCv3Ur1/fo2MCQFEIswDggSuvvFIdO3bUzJkzderUKUlSUFCQRo8erR07dujRRx8tMGbp0qWaO3euYmNj9Y9//MM5ZsyYMdq2bZvGjBlT6B3Td999V99//32RtXTu3Fm9evXS66+/ro8++qjA93l5eRo9erRzu1GjRtq+fbuOHDnibPvpp5+0du1at89fkmrUqKHY2FgtWLBA8+fPl91uL3B3ecCAAVq3bp2WL19eYPyxY8d05swZj44JAEXhDWAAUIizbwD74YcfnNMMzlq0aJFuvfVWvfzyy7r33nsl/flP9XFxcfrggw/UvXt39e/fX4GBgVqzZo3effddNWvWTKtWrXJ5A5jD4dCdd96pd955R23btnW+ASw9PV0fffSRvv/+e33zzTfq3LlzkXUeOXJE1157rX766Sf17t1bV199tapWrapffvlF8+fPV1pamnJzcyX9ufpBixYt1KpVKw0dOlS///67Zs+erbCwMGVlZTmXHdu3b58aNGigKVOmuIThv5o3b57++c9/qnr16rryyiudy4SddfLkSXXr1k2bNm3SnXfeqXbt2ik7O1ubN2/WokWLtG/fPpdpCQBQYt59ZwMAVExFvQHMGGPy8/NNo0aNTKNGjcyZM2dc2t98803TtWtXExwcbAICAsxll11mkpKSzIkTJ4o81qJFi8y1115ratWqZfz8/ExERISJi4szq1evdqvWkydPmqlTp5oOHTqYatWqGbvdbi655BLz4IMPml27drn0fffdd03Dhg2N3W43rVu3NsuXLzfx8fHm4osvdvbZu3evkWSmTJlS5DGzsrJMYGCgkWTefffdQvscP37cjBs3zjRu3NjY7XYTGhpqunTpYqZOnWry8vLcOjcAOBfuzAIAAMCymDMLAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLL8vF1AeXM4HDp06JCqV68um83m7XIAAADwN8YYHT9+XJGRkfLxKf7e6wUXZg8dOqSoqChvlwEAAIBz+O2331SvXr1i+1xwYbZ69eqS/vzNCQ4O9nI1AAAA+LusrCxFRUU5c1txLrgwe3ZqQXBwMGEWAACgAnNnSigPgAEAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLK+G2a+++kq9e/dWZGSkbDabPvroo3OOWb16tdq2bSt/f381btxYc+fOLfM6AQAAUDF5NcxmZ2erVatWmjVrllv99+7dqxtuuEE9e/ZUamqqHnroId11111avnx5GVeKii4tM0ff7M5QWmaOt0sBAKDSqcg/Z/28efDrrrtO1113ndv9Z8+erQYNGmjatGmSpGbNmmnNmjWaMWOGYmNjy6pMVHApP+zXuMWb5TCSj01Kuuky9W9Xz9tlAQBQKXyw/oASl/zs/Dmb3K+l4jrU93ZZTl4Ns55at26dYmJiXNpiY2P10EMPFTkmNzdXubm5zu2srKyyKg9ekJaZ4wyykuQw0oSPf9aEj3/2bmEAAFRCDiONX7xF3ZvUVkRIoLfLkWSxB8DS09MVFhbm0hYWFqasrCzl5BR+2zs5OVkhISHOT1RUVHmUinKyNyPbGWQBAEDZyzdG+zJOersMJ0vdmS2JcePGKSEhwbmdlZVFoLWwtMwc7c3IVoPQqooICVSD0Krysckl0PrYpJUJPRQeEuC9QgEAqATSM08pZvqXLj9nfW02RYcGea+ov7FUmA0PD9fhw4dd2g4fPqzg4GAFBhZ+q9vf31/+/v7lUR7KWFFzY69rEaGlm9Oc/W5uU1cNa1fzYqUAAFQODWtXU3K/lhq/eIvyjZGvzabJ/VpUmCkGksXCbOfOnfXpp5+6tK1YsUKdO3f2UkUoL57Mjf1o4yGNjm1aof6gAQBgVXEd6qt7k9ral3FS0aFBFe7nq1fnzJ44cUKpqalKTU2V9OfSW6mpqdq/f7+kP6cIDB482Nn/3nvv1Z49e/TII49o+/bteumll7RgwQI9/PDD3igf5ciTubEVbS4PAABWFxESqM6NLqpwQVbycpj98ccf1aZNG7Vp00aSlJCQoDZt2mjixImSpLS0NGewlaQGDRpo6dKlWrFihVq1aqVp06bp9ddfZ1kui3Nn7bqzc2P/yscmpdz9jwLtFW0uDwAAKDs2Y8wF9Sx4VlaWQkJClJmZqeDgYG+Xc8HzZI3Yfy/c5DI3tn/bupo2oLVSfthfYC5PRVr/DgAAeMaTvEaYhdekZeao69Ofl3hpLV+bTWvG9lRESKDSMnMq7FweAADgGU/ymqUeAEPlcr5rxJ6dGxsREuj8AACACwthFufl7+u+esKTNWKtsM4dAAAof4RZlJgn810LExJYxe01Yq2wzh0AACh/zJlFiZzvfNei/HUebFHHZW4sAACVG3NmUebOd75rUf46D7YwzI0FAAB/RZhFiXgy37UozIMFAADny6svTYD3ufPCgsJEhAQq6abLnNs+Nim5X0s1rF1NQXY/tz5n58H62v586wHzYAEAgKe4M3sBO98HuHLPOJy/LunM64r+vmcAAFCx8QDYBaosHuA618NbAAAA7vAkrzHN4AJVFg9wnX14CwAAoLwwzaCScfclBuf7ABcPbwEAgIqAMFuJeDIH1pMXFhSGlxgAAICKgDmzlURpzIEtyZxXXmIAAABKGy9NuACVxhzYc72woDC8xAAAAHgTYbaS8HQOLHNeAQBAZcBqBpWEpy8x4IUFAACgMmDObCVyMu+Mmk9cLkn6fFQPtx7mYs4rAACoaJgzC7eW15KY8woAAKyNaQYAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMKsBaRl5uib3RlKy8xxe0x65qkyrAgAAKBiYGmuCi7lh/0at3izHObPFyEk3XSZ+rerV2jf977b7/x1zPQvldyvpeI61C+vUgEAAModL02owNIyc9T16c9dXjnrCV+bTWvG9mQdWQAAYCme5DWmGVRgezOySxxkJSnfGO3LOFl6BQEAAFQwTDOoYNIyc7Q3I1sNQquqQWhV+djkEmh9bNLKhB4F3vCVnnlKMdO/dOnra7MpOjSonCoHAAAof4TZCqSw+bHXtYjQ0s1pzj43t6mrhrWrFRjbsHY1JfdrqfGLtyjfGPnabJrcrwVTDAAAQKXGnNkKwt35seeaB5uWmaN9GScVHRpEkAUAAJbkSV7jzmwF4e782LPzYIsKqhEhgYRYAABwweABsAri7PzYv7JJBdqYBwsAAPA/hNkKIiIkUEk3Xebc9rFJT/dvqeR+LeVr+zPRMg8WAADAFdMMKpD+7eppwsc/S/pzxYKzD3p1b1KbebAAAACFIMxWUH9deot5sAAAAIVjmgEAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsy+thdtasWYqOjlZAQIA6deqk77//vtj+M2fOVNOmTRUYGKioqCg9/PDDOnXqVDlVCwAAgIrEq2E2JSVFCQkJSkxM1IYNG9SqVSvFxsbq999/L7T/e++9p7FjxyoxMVHbtm3TG2+8oZSUFI0fP76cKwcAAEBF4NUwO336dA0bNkxDhgxR8+bNNXv2bAUFBWnOnDmF9v/mm2/UtWtXDRw4UNHR0br22mt1++23n/NuLgAAAConr4XZvLw8rV+/XjExMf8rxsdHMTExWrduXaFjunTpovXr1zvD6549e/Tpp5/q+uuvL/I4ubm5ysrKcvkAAACgcvDz1oEzMjKUn5+vsLAwl/awsDBt37690DEDBw5URkaGrrjiChljdObMGd17773FTjNITk5WUlJSqdYOAACAisHrD4B5YvXq1Zo8ebJeeuklbdiwQYsXL9bSpUv1xBNPFDlm3LhxyszMdH5+++23cqwYAAAAZclrd2ZDQ0Pl6+urw4cPu7QfPnxY4eHhhY6ZMGGC7rjjDt11112SpJYtWyo7O1t33323Hn30Ufn4FMzm/v7+8vf3L/0TAAAAgNd57c6s3W5Xu3bttGrVKmebw+HQqlWr1Llz50LHnDx5skBg9fX1lSQZY8quWAAAAFRIXrszK0kJCQmKj49X+/bt1bFjR82cOVPZ2dkaMmSIJGnw4MGqW7eukpOTJUm9e/fW9OnT1aZNG3Xq1Em7du3ShAkT1Lt3b2eoBQAAwIXDq2E2Li5OR44c0cSJE5Wenq7WrVtr2bJlzofC9u/f73In9rHHHpPNZtNjjz2mgwcPqnbt2urdu7eeeuopb50CAAAAvMhmLrB/n8/KylJISIgyMzMVHBzs7XJcnMw7o+YTl0uStk6KVZDdq3/XAAAA8ApP8pqlVjMAAAAA/oowCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIsw6yVpmTn6ZneG0jJzCv0+PfNUOVcEAABgPX7eLuBClPLDfo1bvFkOI/nYpKSbLlP/dvX03nf7nX1ipn+p5H4tFdehvhcrBQAAqNhsxhjj7SLKU1ZWlkJCQpSZmang4OByP35aZo66Pv25HG78rvvabFoztqciQgLLvjAAAIAKwpO8xjSDcrY3I9utICtJ+cZoX8bJsi0IAADAwgiz5axBaFX52FzbfGxSyt3/KNDua7MpOjSo/IoDAACwGMJsOYsICVTSTZc5t31sUnK/lurU8CIl92spX9ufidbXZtPkfi2YYgAAAFAM5sx6wcm8M2o+cbkk6fNRPdSwdjXnd2mZOdqXcVLRoUEEWQAAcEHyJK+xmoGXhYcEuGxHhAQSYgEAANzENAMAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABY1nmF2VOnTpVWHQAAAIDHPA6zDodDTzzxhOrWratq1appz549kqQJEybojTfeKPUCAQAAgKJ4HGaffPJJzZ07V88++6zsdruzvUWLFnr99ddLtTgAAACgOB6H2bfffluvvvqqBg0aJF9fX2d7q1attH379lItDgAAACiOx2H24MGDaty4cYF2h8Oh06dPl0pRAAAAgDs8DrPNmzfX119/XaB90aJFatOmTakUBQAAALjDz9MBEydOVHx8vA4ePCiHw6HFixdrx44devvtt/XJJ5+URY0AAABAoTy+M9unTx/95z//0cqVK1W1alVNnDhR27Zt03/+8x9dc801ZVEjAAAAUCiP78xKUrdu3bRixYrSrgUAAADwiMd3Zhs2bKj//ve/BdqPHTumhg0blkpRAAAAgDs8DrP79u1Tfn5+gfbc3FwdPHiwVIoCAAAA3OH2NIMlS5Y4f718+XKFhIQ4t/Pz87Vq1SpFR0eXanEAAABAcdwOs3379pUk2Ww2xcfHu3xXpUoVRUdHa9q0aaVaHAAAAFAct8Osw+GQJDVo0EA//PCDQkNDy6woAAAAwB0er2awd+/esqgDAAAA8FiJlubKzs7Wl19+qf379ysvL8/luxEjRni0r1mzZmnKlClKT09Xq1at9MILL6hjx45F9j927JgeffRRLV68WEePHtXFF1+smTNn6vrrry/JqQAAAMDCPA6zGzdu1PXXX6+TJ08qOztbtWrVUkZGhoKCglSnTh2PwmxKSooSEhI0e/ZsderUSTNnzlRsbKx27NihOnXqFOifl5ena665RnXq1NGiRYtUt25d/frrr6pRo4anpwEAAIBKwOOluR5++GH17t1bf/zxhwIDA/Xtt9/q119/Vbt27TR16lSP9jV9+nQNGzZMQ4YMUfPmzTV79mwFBQVpzpw5hfafM2eOjh49qo8++khdu3ZVdHS0evTooVatWnl6GgAAAKgEPA6zqampGjVqlHx8fOTr66vc3FxFRUXp2Wef1fjx493eT15entavX6+YmJj/FePjo5iYGK1bt67QMUuWLFHnzp31wAMPKCwsTC1atNDkyZMLXff2rNzcXGVlZbl8AAAAUDl4HGarVKkiH58/h9WpU0f79++XJIWEhOi3335zez8ZGRnKz89XWFiYS3tYWJjS09MLHbNnzx4tWrRI+fn5+vTTTzVhwgRNmzZNTz75ZJHHSU5OVkhIiPMTFRXldo0AAACo2DyeM9umTRv98MMPuuSSS9SjRw9NnDhRGRkZeuedd9SiRYuyqNHJ4XCoTp06evXVV+Xr66t27drp4MGDmjJlihITEwsdM27cOCUkJDi3s7KyCLQAAACVhMd3ZidPnqyIiAhJ0lNPPaWaNWvqvvvu05EjR/TKK6+4vZ/Q0FD5+vrq8OHDLu2HDx9WeHh4oWMiIiLUpEkT+fr6OtuaNWum9PT0AqsqnOXv76/g4GCXDwAAACoHj8Ns+/bt1bNnT0l/TjNYtmyZsrKytH79erVu3drt/djtdrVr106rVq1ytjkcDq1atUqdO3cudEzXrl21a9cu5wscJGnnzp2KiIiQ3W739FQAAABgcR6H2aJs2LBBN954o0djEhIS9Nprr+mtt97Stm3bdN999yk7O1tDhgyRJA0ePFjjxo1z9r/vvvt09OhRjRw5Ujt37tTSpUs1efJkPfDAA6V1GgAAALAQj+bMLl++XCtWrJDdbtddd92lhg0bavv27Ro7dqz+85//KDY21qODx8XF6ciRI5o4caLS09PVunVrLVu2zPlQ2P79+50Pm0lSVFSUli9frocffliXX3656tatq5EjR2rMmDEeHRcAAACVg80YY9zp+MYbb2jYsGGqVauW/vjjD1100UWaPn26HnzwQcXFxWnkyJFq1qxZWdd73rKyshQSEqLMzEyvzZ89mXdGzSculyRtnRSrIHuJXsQGAABQKXmS19yeZvDcc8/pmWeeUUZGhhYsWKCMjAy99NJL2rx5s2bPnm2JIAsAAIDKxe0wu3v3bt16662SpH79+snPz09TpkxRvXr1yqw4AAAAoDhuh9mcnBwFBQVJkmw2m/z9/Z1LdAEAAADe4NFkzddff13VqlWTJJ05c0Zz585VaGioS58RI0aUXnUAAABAMdx+ACw6Olo2m634ndls2rNnT6kUVlZ4AAwAAKBi8ySvuZ2i9u3bd751AQAAAKWq1F6aAAAAAJQ3wiwAAAAsizALAAAAyyLMAgAAwLIIswAAALCsEoXZ3bt367HHHtPtt9+u33//XZL02Wef6eeffy7V4gAAAIDieBxmv/zyS7Vs2VLfffedFi9erBMnTkiSfvrpJyUmJpZ6gQAAAEBRPA6zY8eO1ZNPPqkVK1bIbrc726+66ip9++23pVocAAAAUByPw+zmzZt18803F2ivU6eOMjIySqUoAAAAwB0eh9kaNWooLS2tQPvGjRtVt27dUikKAAAAcIfHYfa2227TmDFjlJ6eLpvNJofDobVr12r06NEaPHhwWdQIAAAAFMrjMDt58mRdeumlioqK0okTJ9S8eXN1795dXbp00WOPPVYWNVpaWmaOvtmdobTMnEK/T888Vc4VAQAAVB42Y4wpycD9+/dry5YtOnHihNq0aaNLLrmktGsrE1lZWQoJCVFmZqaCg4PL9FgpP+zXuMWb5TCSj01Kuuky9W9XT+99t19PLt0m6c/25H4tFdehfpnWAgAAYBWe5DWPw+yaNWt0xRVXnFeB3lReYTYtM0ddn/5cDjd+d31tNq0Z21MRIYFlVg8AAIBVeJLXPJ5mcNVVV6lBgwYaP368tm7dWuIiK7u9GdluBVlJyjdG+zJOlm1BAAAAlZDHYfbQoUMaNWqUvvzyS7Vo0UKtW7fWlClTdODAgbKoz7IahFaVj821zccmpdz9jwLtvjabokODyq84AACASsLjMBsaGqrhw4dr7dq12r17t2699Va99dZbio6O1lVXXVUWNVpSREigkm66zLl9dm5sp4YXKblfS/na/ky0vjabJvdrwRQDAACAEijxA2Bn5efn67PPPtOECRO0adMm5efnl1ZtZaI8HwA7mXdGzSculyR9PqqHGtau5vwuLTNH+zJOKjo0iCALAADwF2U6Z/astWvX6v7771dERIQGDhyoFi1aaOnSpSXdXaUXHhLgsh0REqjOjS4iyAIAAJwHP08HjBs3TvPnz9ehQ4d0zTXX6LnnnlOfPn0UFMScTwAAAJQvj8PsV199pX//+98aMGCAQkNDy6ImAAAAwC0eh9m1a9eWRR0AAACAx9wKs0uWLNF1112nKlWqaMmSJcX2vemmm0qlMAAAAOBc3Aqzffv2VXp6uurUqaO+ffsW2c9ms1X41QwAAABQebgVZh0OR6G/BgAAALzJ46W53n77beXm5hZoz8vL09tvv10qRQEAAADu8DjMDhkyRJmZmQXajx8/riFDhpRKUQAAAIA7PA6zxhjZ/v+rWP/qwIEDCgkJKZWiAAAAAHe4vTRXmzZtZLPZZLPZdPXVV8vP739D8/PztXfvXvXq1atMigQAAAAK43aYPbuKQWpqqmJjY1WtWjXnd3a7XdHR0erfv3+pFwgAAAAUxe0wm5iYKEmKjo5WXFycAgICyqwoAAAAwB0evwEsPj6+LOoAAAAAPOZWmK1Vq5Z27typ0NBQ1axZs9AHwM46evRoqRUHAAAAFMetMDtjxgxVr17d+eviwiwAAABQXtwKs3+dWnDnnXeWVS0AAACARzxeZ3bDhg3avHmzc/vjjz9W3759NX78eOXl5ZVqcQAAAEBxPA6z99xzj3bu3ClJ2rNnj+Li4hQUFKSFCxfqkUceKfUCAQAAgKJ4HGZ37typ1q1bS5IWLlyoHj166L333tPcuXP1wQcflHZ9AAAAQJFK9Dpbh8MhSVq5cqWuv/56SVJUVJQyMjJKtzoAAACgGB6H2fbt2+vJJ5/UO++8oy+//FI33HCDJGnv3r0KCwsr9QIBAACAongcZmfOnKkNGzZo+PDhevTRR9W4cWNJ0qJFi9SlS5dSLxAAAAAoisdvALv88stdVjM4a8qUKfL19S2VogAAAAB3eBxmz1q/fr22bdsmSWrevLnatm1bakUBAAAA7vA4zP7++++Ki4vTl19+qRo1akiSjh07pp49e2r+/PmqXbt2adcIAAAAFMrjObMPPvigTpw4oZ9//llHjx7V0aNHtWXLFmVlZWnEiBFlUSMAAABQKI/vzC5btkwrV65Us2bNnG3NmzfXrFmzdO2115ZqcQAAAEBxPL4z63A4VKVKlQLtVapUca4/CwAAAJQHj8PsVVddpZEjR+rQoUPOtoMHD+rhhx/W1VdfXarFAQAAAMXxOMy++OKLysrKUnR0tBo1aqRGjRqpQYMGysrK0gsvvFAWNQIAAACF8njObFRUlDZs2KBVq1Y5l+Zq1qyZYmJiSr04AAAAoDgehdmUlBQtWbJEeXl5uvrqq/Xggw+WVV0AAADAObkdZl9++WU98MADuuSSSxQYGKjFixdr9+7dmjJlSlnWBwAAABTJ7TmzL774ohITE7Vjxw6lpqbqrbfe0ksvvVSWtQEAAADFcjvM7tmzR/Hx8c7tgQMH6syZM0pLSyuTwgAAAIBzcTvM5ubmqmrVqv8b6OMju92unJycMikMAAAAOBePHgCbMGGCgoKCnNt5eXl66qmnFBIS4mybPn166VUHAAAAFMPtMNu9e3ft2LHDpa1Lly7as2ePc9tms5VeZQAAAMA5uB1mV69eXYZlAAAAAJ7z+A1gAAAAQEVBmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZVojD79ddf65///Kc6d+6sgwcPSpLeeecdrVmzplSLAwAAAIrjcZj94IMPFBsbq8DAQG3cuFG5ubmSpMzMTE2ePLnUCwQAAACK4nGYffLJJzV79my99tprqlKlirO9a9eu2rBhQ6kWBwAAABTH4zC7Y8cOde/evUB7SEiIjh07Vho1AQAAAG7xOMyGh4dr165dBdrXrFmjhg0blqiIWbNmKTo6WgEBAerUqZO+//57t8bNnz9fNptNffv2LdFxAQAAYG0eh9lhw4Zp5MiR+u6772Sz2XTo0CHNmzdPo0eP1n333edxASkpKUpISFBiYqI2bNigVq1aKTY2Vr///nux4/bt26fRo0erW7duHh8TAAAAlYPHYXbs2LEaOHCgrr76ap04cULdu3fXXXfdpXvuuUcPPvigxwVMnz5dw4YN05AhQ9S8eXPNnj1bQUFBmjNnTpFj8vPzNWjQICUlJZX4bjAAAACsz+Mwa7PZ9Oijj+ro0aPasmWLvv32Wx05ckRPPPGExwfPy8vT+vXrFRMT87+CfHwUExOjdevWFTlu0qRJqlOnjoYOHXrOY+Tm5iorK8vlAwAAgMrBr6QD7Xa7mjdvfl4Hz8jIUH5+vsLCwlzaw8LCtH379kLHrFmzRm+88YZSU1PdOkZycrKSkpLOq04AAABUTB6H2Z49e8pmsxX5/eeff35eBRXn+PHjuuOOO/Taa68pNDTUrTHjxo1TQkKCczsrK0tRUVFlVSIAAADKkcdhtnXr1i7bp0+fVmpqqrZs2aL4+HiP9hUaGipfX18dPnzYpf3w4cMKDw8v0H/37t3at2+fevfu7WxzOBySJD8/P+3YsUONGjVyGePv7y9/f3+P6gIAAIA1eBxmZ8yYUWj7448/rhMnTni0L7vdrnbt2mnVqlXO5bUcDodWrVql4cOHF+h/6aWXavPmzS5tjz32mI4fP67nnnuOO64AAAAXmBLPmf27f/7zn+rYsaOmTp3q0biEhATFx8erffv26tixo2bOnKns7GwNGTJEkjR48GDVrVtXycnJCggIUIsWLVzG16hRQ5IKtAMAAKDyK7Uwu27dOgUEBHg8Li4uTkeOHNHEiROVnp6u1q1ba9myZc6Hwvbv3y8fH48XXQAAAMAFwOMw269fP5dtY4zS0tL0448/asKECSUqYvjw4YVOK5Ck1atXFzt27ty5JTomAAAArM/jMBsSEuKy7ePjo6ZNm2rSpEm69tprS60wAAAA4Fw8CrP5+fkaMmSIWrZsqZo1a5ZVTQAAAIBbPJqM6uvrq2uvvVbHjh0ro3IAAAAA93n8ZFWLFi20Z8+esqgFAAAA8IjHYfbJJ5/U6NGj9cknnygtLU1ZWVkuHwAAAKC8uD1ndtKkSRo1apSuv/56SdJNN93k8lpbY4xsNpvy8/NLv0oAAACgEG6H2aSkJN1777364osvyrIeAAAAwG1uh1ljjCSpR48eZVYMAAAA4AmP5sz+dVoBAAAA4G0erTPbpEmTcwbao0ePnldBAAAAgLs8CrNJSUkF3gAGAAAAeItHYfa2225TnTp1yqoWAAAAwCNuz5llviwAAAAqGrfD7NnVDAAAAICKwu1pBg6HoyzrAAAAADzm8etsAQAAgIqCMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLqhBhdtasWYqOjlZAQIA6deqk77//vsi+r732mrp166aaNWuqZs2aiomJKbY/AAAAKi+vh9mUlBQlJCQoMTFRGzZsUKtWrRQbG6vff/+90P6rV6/W7bffri+++ELr1q1TVFSUrr32Wh08eLCcKwcAAIC32YwxxpsFdOrUSR06dNCLL74oSXI4HIqKitKDDz6osWPHnnN8fn6+atasqRdffFGDBw8+Z/+srCyFhIQoMzNTwcHB511/cU7mnVHzicslSVsnxSrI7lemxwMAAKgMPMlrXr0zm5eXp/Xr1ysmJsbZ5uPjo5iYGK1bt86tfZw8eVKnT59WrVq1Cv0+NzdXWVlZLh8AAABUDl4NsxkZGcrPz1dYWJhLe1hYmNLT093ax5gxYxQZGekSiP8qOTlZISEhzk9UVNR51w0AAICKwetzZs/H008/rfnz5+vDDz9UQEBAoX3GjRunzMxM5+e3334r5yoBAABQVrw6iTM0NFS+vr46fPiwS/vhw4cVHh5e7NipU6fq6aef1sqVK3X55ZcX2c/f31/+/v6lUi8AAAAqFq/embXb7WrXrp1WrVrlbHM4HFq1apU6d+5c5Lhnn31WTzzxhJYtW6b27duXR6kAAACogLz+eH1CQoLi4+PVvn17dezYUTNnzlR2draGDBkiSRo8eLDq1q2r5ORkSdIzzzyjiRMn6r333lN0dLRzbm21atVUrVo1r50HAAAAyp/Xw2xcXJyOHDmiiRMnKj09Xa1bt9ayZcucD4Xt379fPj7/u4H88ssvKy8vT7fccovLfhITE/X444+XZ+kAAADwMq+vM1veWGcWAACgYrPMOrMAAADA+SDMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAy/LzdgEAAABn5efn6/Tp094uA+WgSpUq8vX1Pe/9EGYBAECFcOLECR04cEDGGG+XgnJgs9lUr149VatW7bz2Q5gFAABel5+frwMHDigoKEi1a9eWzWbzdkkoQ8YYHTlyRAcOHNAll1xyXndoCbMAAMDrTp8+LWOMateurcDAQG+Xg3JQu3Zt7du3T6dPnz6vMMsDYOUkPfOUt0sAAKDC447shaO0rjVhtgx9sP6A89cx079Uyg/7vVgNAABA5UOYLSNpmTlKXPKzc9thpPGLtygtM8eLVQEAAFQuhNkysjcjW46/PYyZb4z2ZZz0TkEAAACVUIUIs7NmzVJ0dLQCAgLUqVMnff/998X2X7hwoS699FIFBASoZcuW+vTTT8upUvc1CK0qn79NBfG12RQdGuSdggAAQKm78847ZbPZZLPZVKVKFTVo0ECPPPKITp0q+KzMJ598oh49eqh69eoKCgpShw4dNHfu3EL3+8EHH+jKK69USEiIqlWrpssvv1yTJk3S0aNHi63niy++0PXXX6+LLrpIQUFBat68uUaNGqWDBw+WxulWSF4PsykpKUpISFBiYqI2bNigVq1aKTY2Vr///nuh/b/55hvdfvvtGjp0qDZu3Ki+ffuqb9++2rJlSzlXXryIkEAl3XSZc9vHJk3u10IRITyhCQBAWUrLzNE3uzPKbWpfr169lJaWpj179mjGjBl65ZVXlJiY6NLnhRdeUJ8+fdS1a1d999132rRpk2677Tbde++9Gj16tEvfRx99VHFxcerQoYM+++wzbdmyRdOmTdNPP/2kd955p8g6XnnlFcXExCg8PFwffPCBtm7dqtmzZyszM1PTpk0r8fnl5eWVeGy5MF7WsWNH88ADDzi38/PzTWRkpElOTi60/4ABA8wNN9zg0tapUydzzz33uHW8zMxMI8lkZmaWvGg3vf3NXnPxmE/MxWM+MdFjPjHzv/+1zI8JAIAV5eTkmK1bt5qcnBxjjDEOh8Nk5572+PP2N3tNg7F//uxtMPYT8/Y3ez3eh8PhcLvu+Ph406dPH5e2fv36mTZt2ji39+/fb6pUqWISEhIKjH/++eeNJPPtt98aY4z57rvvjCQzc+bMQo/3xx9/FNr+22+/Gbvdbh566KFixyUmJppWrVq5fDdjxgxz8cUXFzinJ5980kRERJjo6Ggzbtw407FjxwL7vfzyy01SUpJz+7XXXjOXXnqp8ff3N02bNjWzZs0qtB5jCl7zv/Ikr3l1ndm8vDytX79e48aNc7b5+PgoJiZG69atK3TMunXrlJCQ4NIWGxurjz76qND+ubm5ys3NdW5nZWWdf+Fu+PsDYEZ/PgDWvUlt7s4CAHAOOafz1Xzi8vPah8NIEz7+WRM+/vncnf9i66RYBdlLFpG2bNmib775RhdffLGzbdGiRTp9+nSBO7CSdM8992j8+PF6//331alTJ82bN0/VqlXT/fffX+j+a9SoUWj7woULlZeXp0ceecSjcUVZtWqVgoODtWLFCmdbcnKydu/erUaNGkmSfv75Z23atEkffPCBJGnevHmaOHGiXnzxRbVp00YbN27UsGHDVLVqVcXHx3t0fE94NcxmZGQoPz9fYWFhLu1hYWHavn17oWPS09ML7Z+enl5o/+TkZCUlJZVOwR4o7gEwwiwAAJXHJ598omrVqunMmTPKzc2Vj4+PXnzxRef3O3fuVEhIiCIiIgqMtdvtatiwoXbu3ClJ+uWXX9SwYUNVqVLFoxp++eUXBQcHF3qMkqhatapef/112e12Z1urVq303nvvacKECZL+DK+dOnVS48aNJUmJiYmaNm2a+vXrJ0lq0KCBtm7dqldeeaXyhtnyMG7cOJc7uVlZWYqKiirz4559AOyvgZYHwAAAcE9gFV9tnRTr0Zj0zFOKmf6ly89eH5u0MqGHwkMCPDq2J3r27KmXX35Z2dnZmjFjhvz8/NS/f3+P9nGWMebcnYoYV5ovnGjZsqVLkJWkQYMGac6cOZowYYKMMXr//fedGSs7O1u7d+/W0KFDNWzYMOeYM2fOKCQkpNTqKoxXw2xoaKh8fX11+PBhl/bDhw8rPDy80DHh4eEe9ff395e/v3/pFOyBiJBAJfdrqfGLtyjfGPnabDwABgCAm2w2m8f/1N+wdrVCf/Y2rF2tjKr8U9WqVZ13J+fMmaNWrVrpjTfe0NChQyVJTZo0UWZmpg4dOqTIyEiXsXl5edq9e7d69uzp7LtmzRqdPn3ao7uzZ4+RlpZW7N1ZHx+fAoH59OnThZ7T391+++0aM2aMNmzYoJycHP3222+Ki4uTJJ04cUKS9Nprr6lTp04u487nVbXu8OpqBna7Xe3atdOqVaucbQ6HQ6tWrVLnzp0LHdO5c2eX/pK0YsWKIvt7U1yH+loztqfeH/YPrRnbU3Ed6nu7JAAAKjVv/+z18fHR+PHj9dhjjykn58/VFPr3768qVaoUuqLA7NmzlZ2drdtvv12SNHDgQJ04cUIvvfRSofs/duxYoe233HKL7Ha7nn322WLH1a5dW+np6S6BNjU11a1zq1evnnr06KF58+Zp3rx5uuaaa1SnTh1Jf075jIyM1J49e9S4cWOXT4MGDdzaf0l5fZpBQkKC4uPj1b59e3Xs2FEzZ85Udna2hgwZIkkaPHiw6tatq+TkZEnSyJEj1aNHD02bNk033HCD5s+frx9//FGvvvqqN0+jSBEhgdyNBQCgHHn7Z++tt96qf//735o1a5ZGjx6t+vXr69lnn9WoUaMUEBCgO+64Q1WqVNHHH3+s8ePHa9SoUc67mZ06ddIjjzziXBv25ptvVmRkpHbt2qXZs2friiuu0MiRIwscMyoqSjNmzNDw4cOVlZWlwYMHKzo6WgcOHNDbb7+tatWqadq0abryyit15MgRPfvss7rlllu0bNkyffbZZwoODnbr3AYNGqTExETl5eVpxowZLt8lJSVpxIgRCgkJUa9evZSbm6sff/xRf/zxR4GH90vVOdc7KAcvvPCCqV+/vrHb7aZjx47O5SmMMaZHjx4mPj7epf+CBQtMkyZNjN1uN5dddplZunSp28cqz6W5AACAe4pbpqkiK2xpLmOMSU5ONrVr1zYnTpxwtn388cemW7dupmrVqiYgIMC0a9fOzJkzp9D9pqSkmO7du5vq1aubqlWrmssvv9xMmjSpyKW5zlqxYoWJjY01NWvWNAEBAebSSy81o0ePNocOHXL2efnll01UVJSpWrWqGTx4sHnqqacKXZqrMH/88Yfx9/c3QUFB5vjx4wW+nzdvnmndurWx2+2mZs2apnv37mbx4sWF7qu0luayGVPCmcYWlZWVpZCQEGVmZrr9txAAAFC2Tp06pb1796pBgwYKCHD/YS1YV3HX3JO85vU3gAEAAAAlRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAFQYF9hz6Re00rrWhFkAAOB1Z98SlZeX5+VKUF7OXuvzfUOY11+aAAAA4Ofnp6CgIB05ckRVqlSRjw/32yozh8OhI0eOKCgoSH5+5xdHCbMAAMDrbDabIiIitHfvXv3666/eLgflwMfHR/Xr15fNZjuv/RBmAQBAhWC323XJJZcw1eACYbfbS+UOPGEWAABUGD4+PrwBDB5hQgoAAAAsizALAAAAyyLMAgAAwLIuuDmzZxfozcrK8nIlAAAAKMzZnObOixUuuDB7/PhxSVJUVJSXKwEAAEBxjh8/rpCQkGL72MwF9t44h8OhQ4cOqXr16ue9rpk7srKyFBUVpd9++03BwcFlfjyUPq6h9XENrY9raG1cP+sr72tojNHx48cVGRl5zuW7Lrg7sz4+PqpXr165Hzc4OJg/wBbHNbQ+rqH1cQ2tjetnfeV5Dc91R/YsHgADAACAZRFmAQAAYFmE2TLm7++vxMRE+fv7e7sUlBDX0Pq4htbHNbQ2rp/1VeRreME9AAYAAIDKgzuzAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizpWDWrFmKjo5WQECAOnXqpO+//77Y/gsXLtSll16qgIAAtWzZUp9++mk5VYqieHINX3vtNXXr1k01a9ZUzZo1FRMTc85rjrLn6Z/Ds+bPny+bzaa+ffuWbYE4J0+v4bFjx/TAAw8oIiJC/v7+atKkCf8/9SJPr9/MmTPVtGlTBQYGKioqSg8//LBOnTpVTtXi77766iv17t1bkZGRstls+uijj845ZvXq1Wrbtq38/f3VuHFjzZ07t8zrLJTBeZk/f76x2+1mzpw55ueffzbDhg0zNWrUMIcPHy60/9q1a42vr6959tlnzdatW81jjz1mqlSpYjZv3lzOleMsT6/hwIEDzaxZs8zGjRvNtm3bzJ133mlCQkLMgQMHyrlynOXpNTxr7969pm7duqZbt26mT58+5VMsCuXpNczNzTXt27c3119/vVmzZo3Zu3evWb16tUlNTS3nymGM59dv3rx5xt/f38ybN8/s3bvXLF++3ERERJiHH364nCvHWZ9++ql59NFHzeLFi40k8+GHHxbbf8+ePSYoKMgkJCSYrVu3mhdeeMH4+vqaZcuWlU/Bf0GYPU8dO3Y0DzzwgHM7Pz/fREZGmuTk5EL7DxgwwNxwww0ubZ06dTL33HNPmdaJonl6Df/uzJkzpnr16uatt94qqxJxDiW5hmfOnDFdunQxr7/+uomPjyfMepmn1/Dll182DRs2NHl5eeVVIorh6fV74IEHzFVXXeXSlpCQYLp27VqmdcI97oTZRx55xFx22WUubXFxcSY2NrYMKysc0wzOQ15entavX6+YmBhnm4+Pj2JiYrRu3bpCx6xbt86lvyTFxsYW2R9lqyTX8O9Onjyp06dPq1atWmVVJopR0ms4adIk1alTR0OHDi2PMlGMklzDJUuWqHPnznrggQcUFhamFi1aaPLkycrPzy+vsvH/leT6denSRevXr3dORdizZ48+/fRTXX/99eVSM85fRcozfuV+xEokIyND+fn5CgsLc2kPCwvT9u3bCx2Tnp5eaP/09PQyqxNFK8k1/LsxY8YoMjKywB9qlI+SXMM1a9bojTfeUGpqajlUiHMpyTXcs2ePPv/8cw0aNEiffvqpdu3apfvvv1+nT59WYmJieZSN/68k12/gwIHKyMjQFVdcIWOMzpw5o3vvvVfjx48vj5JRCorKM1lZWcrJyVFgYGC51cKdWeA8PP3005o/f74+/PBDBQQEeLscuOH48eO644479Nprryk0NNTb5aCEHA6H6tSpo1dffVXt2rVTXFycHn30Uc2ePdvbpcENq1ev1uTJk/XSSy9pw4YNWrx4sZYuXaonnnjC26XBgrgzex5CQ0Pl6+urw4cPu7QfPnxY4eHhhY4JDw/3qD/KVkmu4VlTp07V008/rZUrV+ryyy8vyzJRDE+v4e7du7Vv3z717t3b2eZwOCRJfn5+2rFjhxo1alS2RcNFSf4cRkREqEqVKvL19XW2NWvWTOnp6crLy5Pdbi/TmvE/Jbl+EyZM0B133KG77rpLktSyZUtlZ2fr7rvv1qOPPiofH+61VXRF5Zng4OByvSsrcWf2vNjtdrVr106rVq1ytjkcDq1atUqdO3cudEznzp1d+kvSihUriuyPslWSayhJzz77rJ544gktW7ZM7du3L49SUQRPr+Gll16qzZs3KzU11fm56aab1LNnT6WmpioqKqo8y4dK9uewa9eu2rVrl/MvIpK0c+dORUREEGTLWUmu38mTJwsE1rN/MTHGlF2xKDUVKs+U+yNnlcz8+fONv7+/mTt3rtm6dau5++67TY0aNUx6eroxxpg77rjDjB071tl/7dq1xs/Pz0ydOtVs27bNJCYmsjSXl3l6DZ9++mljt9vNokWLTFpamvNz/Phxb53CBc/Ta/h3rGbgfZ5ew/3795vq1aub4cOHmx07dphPPvnE1KlTxzz55JPeOoULmqfXLzEx0VSvXt28//77Zs+ePeb//u//TKNGjcyAAQO8dQoXvOPHj5uNGzeajRs3Gklm+vTpZuPGjebXX381xhgzduxYc8cddzj7n12a69///rfZtm2bmTVrFktzWdkLL7xg6tevb+x2u+nYsaP59ttvnd/16NHDxMfHu/RfsGCBadKkibHb7eayyy4zS5cuLeeK8XeeXMOLL77YSCrwSUxMLP/C4eTpn8O/IsxWDJ5ew2+++cZ06tTJ+Pv7m4YNG5qnnnrKnDlzppyrxlmeXL/Tp0+bxx9/3DRq1MgEBASYqKgoc//995s//vij/AuHMcaYL774otCfbWevW3x8vOnRo0eBMa1btzZ2u900bNjQvPnmm+VetzHG2Izhfj4AAACsiTmzAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizACBp7ty5qlGjhrfLKDGbzaaPPvqo2D533nmn+vbtWy71AEB5IcwCqDTuvPNO2Wy2Ap9du3Z5uzTNnTvXWY+Pj4/q1aunIUOG6Pfffy+V/aelpem6666TJO3bt082m02pqakufZ577jnNnTu3VI5XlMcff9x5nr6+voqKitLdd9+to0ePerQfgjcAd/l5uwAAKE29evXSm2++6dJWu3ZtL1XjKjg4WDt27JDD4dBPP/2kIUOG6NChQ1q+fPl57zs8PPycfUJCQs77OO647LLLtHLlSuXn52vbtm3617/+pczMTKWkpJTL8QFcWLgzC6BS8ff3V3h4uMvH19dX06dPV8uWLVW1alVFRUXp/vvv14kTJ4rcz08//aSePXuqevXqCg4OVrt27fTjjz86v1+zZo26deumwMBARUVFacSIEcrOzi62NpvNpvDwcEVGRuq6667TiBEjtHLlSuXk5MjhcGjSpEmqV6+e/P391bp1ay1btsw5Ni8vT8OHD1dERIQCAgJ08cUXKzk52WXfZ6cZNGjQQJLUpk0b2Ww2XXnllZJc73a++uqrioyMlMPhcKmxT58++te//uXc/vjjj9W2bVsFBASoYcOGSkpK0pkzZ4o9Tz8/P4WHh6tu3bqKiYnRrbfeqhUrVji/z8/P19ChQ9WgQQMFBgaqadOmeu6555zfP/7443rrrbf08ccfO+/yrl69WpL022+/acCAAapRo4Zq1aqlPn36aN++fcXWA6ByI8wCuCD4+Pjo+eef188//6y33npLn3/+uR555JEi+w8aNEj16tXTDz/8oPXr12vs2LGqUqWKJGn37t3q1auX+vfvr02bNiklJUVr1qzR8OHDPaopMDBQDodDZ86c0XPPPadp06Zp6tSp2rRpk2JjY3XTTTfpl19+kSQ9//zzWrJkiRYsWKAdO3Zo3rx5io6OLnS/33//vSRp5cqVSktL0+LFiwv0ufXWW/Xf//5XX3zxhbPt6NGjWrZsmQYNGiRJ+vrrrzV48GCNHDlSW7du1SuvvKK5c+fqqaeecvsc9+3bp+XLl8tutzvbHA6H6tWrp4ULF2rr1q2aOHGixo8frwULFkiSRo8erQEDBqhXr15KS0tTWlqaunTpotOnTys2NlbVq1fX119/rbVr16patWrq1auX8vLy3K4JQCVjAKCSiI+PN76+vqZq1arOzy233FJo34ULF5qLLrrIuf3mm2+akJAQ53b16tXN3LlzCx07dOhQc/fdd7u0ff3118bHx8fk5OQUOubv+9+5c6dp0qSJad++vTHGmMjISPPUU0+5jOnQoYO5//77jTHGPPjgg+aqq64yDoej0P1LMh9++KExxpi9e/caSWbjxo0ufeLj402fPn2c23369DH/+te/nNuvvPKKiYyMNPn5+cYYY66++mozefJkl3288847JiIiotAajDEmMTHR+Pj4mKpVq5qAgAAjyUgy06dPL3KMMcY88MADpn///kXWevbYTZs2dfk9yM3NNYGBgWb58uXF7h9A5cWcWQCVSs+ePfXyyy87t6tWrSrpz7uUycnJ2r59u7KysnTmzBmdOnVKJ0+eVFBQUIH9JCQk6K677tI777zj/KfyRo0aSfpzCsKmTZs0b948Z39jjBwOh/bu3atmzZoVWltmZqaqVasmh8OhU6dO6YorrtDrr7+urKwsHTp0SF27dnXp37VrV/3000+S/pwicM0116hp06bq1auXbrzxRl177bXn9Xs1aNAgDRs2TC+99JL8/f01b9483XbbbfLx8XGe59q1a13uxObn5xf7+yZJTZs21ZIlS3Tq1Cm9++67Sk1N1YMPPujSZ9asWZozZ47279+vnJwc5eXlqXXr1sXW+9NPP2nXrl2qXr26S/upU6e0e/fuEvwOAKgMCLMAKpWqVauqcePGLm379u3TjTfeqPvuu09PPfWUatWqpTVr1mjo0KHKy8srNJQ9/vjjGjhwoJYuXarPPvtMiYmJmj9/vm6++WadOHFC99xzj0aMGFFgXP369YusrXr16tqwYYN8fHwUERGhwMBASVJWVtY5z6tt27bau3evPvvsM61cuVIDBgxQTEyMFi1adM6xRendu7eMMVq6dKk6dOigr7/+WjNmzHB+f+LECSUlJalfv34FxgYEBBS5X7vd7rwGTz/9tG644QYlJSXpiSeekCTNnz9fo0eP1rRp09S5c2dVr15dU6ZM0XfffVdsvSdOnFC7du1c/hJxVkV5yA9A+SPMAqj01q9fL4fDoWnTpjnvOp6dn1mcJk2aqEmTJnr44Yd1++23680339TNN9+stm3bauvWrQVC87n4+PgUOiY4OFiRkZFau3atevTo4Wxfu3atOnbs6NIvLi5OcXFxuuWWW9SrVy8dPXpUtWrVctnf2fmp+fn5xdYTEBCgfv36ad68edq1a5eaNm2qtm3bOr9v27atduzY4fF5/t1jjz2mq666Svfdd5/zPLt06aL777/f2efvd1btdnuB+tu2bauUlBTVqVNHwcHB51UTgMqDB8AAVHqNGzfW6dOn9cILL2jPnj165513NHv27CL75+TkaPjw4Vq9erV+/fVXrV27Vj/88INz+sCYMWP0zTffaPjw4UpNTdUvv/yijz/+2OMHwP7q3//+t5555hmlpKRox44dGjt2rFJTUzVy5EhJ0vTp0/X+++9r+/bt2rlzpxYuXKjw8PBCX/RQp04dBQYGatmyZTp8+LAyMzOLPO6gQYO0dOlSzZkzx/ng11kTJ07U22+/raSkJP3888/atm2b5s+fr8cee8yjc+vcubMuv/xyTZ48WZJ0ySWX6Mcff9Ty5cu1c+dOTZgwQT/88IPLmOjoaG3atEk7duxQRkaGTp8+rUGDBik0NFR9+vTR119/rb1792r16tUaMWKEDhw44FFNACoPwiyASq9Vq1aaPn26nnnmGbVo0ULz5s1zWdbq73x9ffXf//5XgwcPVpMmTTRgwABdd911SkpKkiRdfvnl+vLLL7Vz505169ZNbdq00cSJExUZGVniGkeMGKGEhASNGjVKLVu21LJly7RkyRJdcsklkv6covDss8+qffv26tChg/bt26dPP/3Ueaf5r/z8/PT888/rlVdeUWRkpPr06VPkca+66irVqlVLO3bs0MCBA12+i42N1SeffKL/+7//U4cOHfSPf/xDM2bM0MUXX+zx+T388MN6/fXX9dtvv+mee+5Rv379FBcXp06dOum///2vy11aSRo2bJiaNm2q9u3bq3bt2lq7dq2CgoL01VdfqX79+urXr5+aNWumoUOH6tSpU9ypBS5gNmOM8XYRAAAAQElwZxYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFn/D3mEZv9Y5d/JAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area under the ROC Curve (AUC-ROC): 0.9863986398639863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "qiFW00UvJ7g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different solvers for Logistic Regression\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Train Logistic Regression models with different solvers and compare their accuracy\n",
        "for solver in solvers:\n",
        "    logreg = LogisticRegression(max_iter=1000, solver=solver)\n",
        "    logreg.fit(X_train, y_train)\n",
        "    y_pred = logreg.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.2f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV0EktOcJ75g",
        "outputId": "ce990bb3-efe0-4598-eb1d-022337a95f03"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Solver: saga, Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Solver: lbfgs, Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)"
      ],
      "metadata": {
        "id": "hr4n5aWjJ8DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, matthews_corrcoef\n",
        "\n",
        "# Generate a binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression object\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model using the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Evaluate the model's performance using Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"\\nMatthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "# Interpret the MCC value\n",
        "if mcc < 0:\n",
        "    print(\"Model performance is worse than random guessing.\")\n",
        "elif mcc == 0:\n",
        "    print(\"Model performance is equal to random guessing.\")\n",
        "elif mcc < 0.3:\n",
        "    print(\"Model performance is slight.\")\n",
        "elif mcc < 0.5:\n",
        "    print(\"Model performance is fair.\")\n",
        "elif mcc < 0.7:\n",
        "    print(\"Model performance is moderate.\")\n",
        "elif mcc < 0.9:\n",
        "    print(\"Model performance is substantial.\")\n",
        "else:\n",
        "    print(\"Model performance is almost perfect.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te5Y2yMMJ8Ng",
        "outputId": "9741e18b-88f7-4e2f-ba09-da0aa079d0d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.94\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94       101\n",
            "           1       0.95      0.93      0.94        99\n",
            "\n",
            "    accuracy                           0.94       200\n",
            "   macro avg       0.94      0.94      0.94       200\n",
            "weighted avg       0.94      0.94      0.94       200\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[96  5]\n",
            " [ 7 92]]\n",
            "\n",
            "Matthews Correlation Coefficient (MCC): 0.8801401405166956\n",
            "Model performance is substantial.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "oFsyajy6J8Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "logreg_raw = LogisticRegression(max_iter=1000)\n",
        "logreg_raw.fit(X_train, y_train)\n",
        "y_pred_raw = logreg_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Accuracy on Raw Data:\", accuracy_raw)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "logreg_scaled = LogisticRegression(max_iter=1000)\n",
        "logreg_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = logreg_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy on Standardized Data:\", accuracy_scaled)\n",
        "\n",
        "# Compare the accuracy of both models\n",
        "print(\"\\nComparison of Accuracy:\")\n",
        "print(\"Raw Data:\", accuracy_raw)\n",
        "print(\"Standardized Data:\", accuracy_scaled)\n",
        "\n",
        "# Print the classification report and confusion matrix for both models\n",
        "print(\"\\nClassification Report on Raw Data:\")\n",
        "print(classification_report(y_test, y_pred_raw))\n",
        "print(\"\\nConfusion Matrix on Raw Data:\")\n",
        "print(confusion_matrix(y_test, y_pred_raw))\n",
        "\n",
        "print(\"\\nClassification Report on Standardized Data:\")\n",
        "print(classification_report(y_test, y_pred_scaled))\n",
        "print(\"\\nConfusion Matrix on Standardized Data:\")\n",
        "print(confusion_matrix(y_test, y_pred_scaled))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjPFvf-0J8jx",
        "outputId": "19645f49-d157-4b20-c42d-c0c27b08b5ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data: 1.0\n",
            "Accuracy on Standardized Data: 1.0\n",
            "\n",
            "Comparison of Accuracy:\n",
            "Raw Data: 1.0\n",
            "Standardized Data: 1.0\n",
            "\n",
            "Classification Report on Raw Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix on Raw Data:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Classification Report on Standardized Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix on Standardized Data:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "Zf5wwsnSJ8tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter space for GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100]}\n",
        "\n",
        "# Perform grid search with cross-validation to find the optimal C\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the optimal C and the corresponding best score\n",
        "print(\"Optimal C:\", grid_search.best_params_['C'])\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "# Train a Logistic Regression model with the optimal C\n",
        "logreg_optimal = LogisticRegression(C=grid_search.best_params_['C'], max_iter=1000)\n",
        "logreg_optimal.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_optimal = logreg_optimal.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy_optimal = accuracy_score(y_test, y_pred_optimal)\n",
        "print(\"Accuracy with Optimal C:\", accuracy_optimal)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(\"\\nClassification Report with Optimal C:\")\n",
        "print(classification_report(y_test, y_pred_optimal))\n",
        "print(\"\\nConfusion Matrix with Optimal C:\")\n",
        "print(confusion_matrix(y_test, y_pred_optimal))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XSs9tNXJ81S",
        "outputId": "85a4bb2a-4a6a-4a35-e5fc-fc8d65a42732"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 1\n",
            "Best Score: 0.9666666666666666\n",
            "Accuracy with Optimal C: 1.0\n",
            "\n",
            "Classification Report with Optimal C:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix with Optimal C:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "yFwvpYT_K0DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(logreg, 'logreg_model.joblib')\n",
        "\n",
        "# Load the saved model using joblib\n",
        "loaded_logreg = joblib.load('logreg_model.joblib')\n",
        "\n",
        "# Make predictions on the testing data using the loaded model\n",
        "y_pred_loaded = loaded_logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the loaded model's accuracy\n",
        "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "print(\"\\nLoaded Model Accuracy:\", accuracy_loaded)\n",
        "\n",
        "# Print the classification report and confusion matrix of the loaded model\n",
        "print(\"\\nClassification Report of Loaded Model:\")\n",
        "print(classification_report(y_test, y_pred_loaded))\n",
        "print(\"\\nConfusion Matrix of Loaded Model:\")\n",
        "print(confusion_matrix(y_test, y_pred_loaded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_18-YDUK0Ld",
        "outputId": "29f44a55-9321-4da6-ac08-32dc7c6275c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Loaded Model Accuracy: 1.0\n",
            "\n",
            "Classification Report of Loaded Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix of Loaded Model:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    }
  ]
}